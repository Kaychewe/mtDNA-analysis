{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mtDNA Heteroplasmy Pipeline — v2 (Comprehensive)\n",
    "\n",
    "## Overview\n",
    "This notebook contains the **comprehensive mtDNA heteroplasmy pipeline**, with the intent to upgrade Stage01–Stage03 to full production versions.\n",
    "\n",
    "### Current Stages (v2)\n",
    "1. **Stage01 (chrM extraction — to be upgraded)**  \n",
    "   Subset WGS CRAM → chrM BAM/BAI/SAM  \n",
    "   Output filename: `<sample>_<age>_<sex>_chrM.bam`\n",
    "\n",
    "2. **Stage02 (mtDNA variant calling)**  \n",
    "   Call mtDNA variants in mitochondria mode  \n",
    "   Output filename: `<sample>_<age>_<sex>_mt.filtered.vcf`\n",
    "\n",
    "3. **Stage03 (filter + summary)**  \n",
    "   Filter by VAF threshold and emit TSV  \n",
    "   Output filename: `<sample>_<age>_<sex>_mt.vaf0.01.tsv`\n",
    "\n",
    "## Output Summary\n",
    "The final TSVs contain:  \n",
    "`CHROM, POS, REF, ALT, QUAL, FILTER, DP, AF, HET`\n",
    "\n",
    "## Goal\n",
    "Aggregate per‑sample heteroplasmy metrics and plot **heteroplasmy vs age** across age bins.\n",
    "\n",
    "## Notes\n",
    "- We will progressively **upgrade Stage01–Stage03** to the full analysis workflow (coverage, contamination, annotation, etc.).\n",
    "- The current implementation is a stable baseline for scaling and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET GLOBAL VARIABLES\n",
    "# NOTE:\n",
    "# The Cloud Life Sciences (GLS) API is expired !\n",
    "# Batch (GCB) migration in the All of Us (AOU) Workbench occurred in July 8, 2025\n",
    "# For migration details:\n",
    "# https://cloud.google.com/batch/docs/migrate-to-batch-from-cloud-life-sciences\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE_BUCKET = os.getenv(\"WORKSPACE_BUCKET\", \"\").rstrip(\"/\")\n",
    "GOOGLE_PROJECT = os.getenv(\"GOOGLE_PROJECT\", \"\")\n",
    "PET_SA_EMAIL = os.getenv(\"PET_SA_EMAIL\", \"\")\n",
    "\n",
    "outputFold = os.getenv(\"outputFold\", \"mtDNA_v25_pilot_5\")\n",
    "PORTID = int(os.getenv(\"PORTID\", \"8094\"))\n",
    "USE_MEM = int(os.getenv(\"USE_MEM\", \"32\"))\n",
    "SQL_DB_NAME = os.getenv(\"SQL_DB_NAME\", \"local_cromwell_run.db\")\n",
    "\n",
    "PROJECT_ROOT = Path(os.getenv(\"PROJECT_ROOT\", \"/mnt/f/research_drive/mtdna/leelab/mtDNA-analysis\")).resolve()\n",
    "\n",
    "print(\"WORKSPACE_BUCKET:\", WORKSPACE_BUCKET)\n",
    "print(\"GOOGLE_PROJECT:\", GOOGLE_PROJECT)\n",
    "print(\"PET_SA_EMAIL:\", PET_SA_EMAIL)\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"PORTID:\", PORTID, \"USE_MEM:\", USE_MEM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def run(cmd, check=True):\n",
    "    print(f\"$ {cmd}\")\n",
    "    return subprocess.run(\n",
    "        cmd, shell=True, check=check,\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True\n",
    "    ).stdout.strip()\n",
    "\n",
    "# --- Dependency checks ---\n",
    "checks = {\n",
    "    \"python\": sys.executable,\n",
    "    \"pip\": shutil.which(\"pip\") or shutil.which(\"pip3\") or \"\",\n",
    "    \"gsutil\": shutil.which(\"gsutil\") or \"\",\n",
    "    \"gcloud\": shutil.which(\"gcloud\") or \"\",\n",
    "    \"java\": shutil.which(\"java\") or \"\",\n",
    "    \"sdkman\": shutil.which(\"sdk\") or \"\",\n",
    "}\n",
    "\n",
    "print(\"Dependency check:\")\n",
    "for k, v in checks.items():\n",
    "    print(f\"  {k:8s} -> {v if v else 'MISSING'}\")\n",
    "\n",
    "if checks[\"java\"]:\n",
    "    print(\"\\nJava version:\")\n",
    "    print(run(\"java -version\", check=False))\n",
    "\n",
    "if checks[\"gcloud\"]:\n",
    "    print(\"\\nGcloud auth:\")\n",
    "    print(run(\"gcloud auth list --format='value(account)'\", check=False))\n",
    "\n",
    "# Optional: install pyhocon if missing\n",
    "try:\n",
    "    import pyhocon  # noqa: F401\n",
    "    print(\"\\npyhocon: OK\")\n",
    "except ImportError:\n",
    "    print(\"\\npyhocon: missing\")\n",
    "    # Uncomment to install\n",
    "    # print(run(f\"{sys.executable} -m pip install pyhocon\", check=True))\n",
    "\n",
    "# --- Optional bootstrap tools (comment out if not needed) ---\n",
    "home = Path.home()\n",
    "sdkman_dir = home / \".sdkman\"\n",
    "\n",
    "if not sdkman_dir.exists():\n",
    "    print(\"\\nInstalling SDKMAN...\")\n",
    "    run(\"curl -s https://get.sdkman.io -o install_sdkman.sh\", check=True)\n",
    "    run(\"bash install_sdkman.sh\", check=True)\n",
    "else:\n",
    "    print(\"\\nSDKMAN already installed.\")\n",
    "\n",
    "sdkman_init = sdkman_dir / \"bin\" / \"sdkman-init.sh\"\n",
    "if sdkman_init.exists():\n",
    "    run(f\"bash -lc 'source {sdkman_init} && sdk install java 17.0.8-tem || true'\", check=True)\n",
    "    run(f\"bash -lc 'source {sdkman_init} && sdk use java 17.0.8-tem'\", check=True)\n",
    "else:\n",
    "    print(\"SDKMAN init script not found; skipping Java install.\")\n",
    "\n",
    "# Cromwell/WOMtool 91\n",
    "if not Path(\"cromwell-91.jar\").exists():\n",
    "    print(\"Downloading cromwell-91.jar\")\n",
    "    run(\"curl -L https://github.com/broadinstitute/cromwell/releases/download/91/cromwell-91.jar -o cromwell-91.jar\", check=True)\n",
    "else:\n",
    "    print(\"cromwell-91.jar already present.\")\n",
    "\n",
    "if not Path(\"womtool-91.jar\").exists():\n",
    "    print(\"Downloading womtool-91.jar\")\n",
    "    run(\"curl -L https://github.com/broadinstitute/cromwell/releases/download/91/womtool-91.jar -o womtool-91.jar\", check=True)\n",
    "else:\n",
    "    print(\"womtool-91.jar already present.\")\n",
    "\n",
    "# Heap size to use in start_cromwell()\n",
    "CROMWELL_HEAP_GB = 32\n",
    "print(\"CROMWELL_HEAP_GB set to\", CROMWELL_HEAP_GB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# ---- Config ----\n",
    "CROMWELL_PORT = 8094\n",
    "CROMWELL_STATUS_URL = f\"http://localhost:{CROMWELL_PORT}/engine/v1/status\"\n",
    "CROMWELL_API = f\"http://localhost:{CROMWELL_PORT}/api/workflows/v1\"\n",
    "CROMWELL_CONF = Path(\"/home/jupyter/cromwell.conf\")\n",
    "CROMWELL_JAR = Path(\"cromwell-91.jar\")  # upgraded (81 was buggy and slow)\n",
    "SDKMAN_INIT = \"/home/jupyter/.sdkman/bin/sdkman-init.sh\"\n",
    "JAVA_VER = \"17.0.8-tem\"\n",
    "CROMWELL_HEAP_GB = 32  # new\n",
    "\n",
    "STDOUT_LOG = Path(\"cromwell_server_stdout.log\")\n",
    "STDERR_LOG = Path(\"cromwell_server_stderr.log\")\n",
    "PID_FILE = Path(\"cromwell_server.pid\")\n",
    "\n",
    "DB_BASE = Path(\"/home/jupyter/cromwell_db/local_cromwell_run.db\")\n",
    "DB_DATA = Path(str(DB_BASE) + \".data\")\n",
    "\n",
    "# ---- Variables ----\n",
    "_last_restart = 0\n",
    "\n",
    "# ---- Helpers ----\n",
    "def cromwell_up():\n",
    "    try:\n",
    "        r = requests.get(CROMWELL_STATUS_URL, timeout=2)\n",
    "        return r.ok\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def cromwell_pid_running():\n",
    "    if not PID_FILE.exists():\n",
    "        return False\n",
    "    try:\n",
    "        pid = int(PID_FILE.read_text().strip())\n",
    "        os.kill(pid, 0)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def cromwell_healthy():\n",
    "    return cromwell_pid_running() and cromwell_up()\n",
    "\n",
    "def cromwell_persistent_ok():\n",
    "    return DB_DATA.exists() and DB_DATA.stat().st_size > 0\n",
    "\n",
    "def cromwell_persistent_recent(max_age_s=300):\n",
    "    if not cromwell_persistent_ok():\n",
    "        return False\n",
    "    age = time.time() - DB_DATA.stat().st_mtime\n",
    "    return age <= max_age_s\n",
    "\n",
    "def start_cromwell():\n",
    "    if cromwell_healthy():\n",
    "        print(\"Cromwell already running and healthy.\")\n",
    "        if cromwell_persistent_ok():\n",
    "            print(\"Persistence check: DB data file OK.\")\n",
    "            if cromwell_persistent_recent():\n",
    "                print(\"DB was updated recently.\")\n",
    "        return\n",
    "\n",
    "    cmd = (\n",
    "        f\"bash -lc 'source {SDKMAN_INIT} && sdk use java {JAVA_VER} && \"\n",
    "        f\"nohup java -Xmx{CROMWELL_HEAP_GB}g \"\n",
    "        f\"-Dconfig.file={CROMWELL_CONF} -Dwebservice.port={CROMWELL_PORT} \"\n",
    "        f\"-jar {CROMWELL_JAR} server > {STDOUT_LOG} 2> {STDERR_LOG} & \"\n",
    "        f\"echo $! > {PID_FILE} && disown'\"\n",
    "    )\n",
    "    print(\"$\", cmd)\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    for _ in range(30):\n",
    "        if cromwell_up():\n",
    "            print(\"Cromwell is up.\")\n",
    "            if cromwell_persistent_ok():\n",
    "                print(\"Persistence check: DB data file OK.\")\n",
    "                if cromwell_persistent_recent():\n",
    "                    print(\"DB was updated recently.\")\n",
    "            return\n",
    "        time.sleep(2)\n",
    "\n",
    "    raise RuntimeError(\"Cromwell did not start. Check stderr/stdout logs.\")\n",
    "\n",
    "def tail_logs(n=50):\n",
    "    if STDOUT_LOG.exists():\n",
    "        print(f\"--- {STDOUT_LOG} (last {n}) ---\")\n",
    "        print(\"\\n\".join(STDOUT_LOG.read_text().splitlines()[-n:]))\n",
    "    else:\n",
    "        print(f\"{STDOUT_LOG} not found.\")\n",
    "    if STDERR_LOG.exists():\n",
    "        print(f\"--- {STDERR_LOG} (last {n}) ---\")\n",
    "        print(\"\\n\".join(STDERR_LOG.read_text().splitlines()[-n:]))\n",
    "    else:\n",
    "        print(f\"{STDERR_LOG} not found.\")\n",
    "\n",
    "def pretty(obj):\n",
    "    print(json.dumps(obj, indent=2))\n",
    "\n",
    "def get_wf_status(wf_id, retries=10, sleep_s=2):\n",
    "    url = f\"{CROMWELL_API}/{wf_id}/status\"\n",
    "    last_err = None\n",
    "    for _ in range(retries):\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code == 404:\n",
    "            time.sleep(sleep_s)\n",
    "            continue\n",
    "        last_err = r\n",
    "        break\n",
    "    if last_err is not None:\n",
    "        last_err.raise_for_status()\n",
    "    raise RuntimeError(f\"Workflow {wf_id} not found after {retries} retries.\")\n",
    "\n",
    "def get_wf_metadata(wf_id, include_keys=None):\n",
    "    url = f\"{CROMWELL_API}/{wf_id}/metadata\"\n",
    "    if include_keys:\n",
    "        for k in include_keys:\n",
    "            url += f\"&includeKey={k}\" if \"?\" in url else f\"?includeKey={k}\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def wait_for_wf(wf_id, poll_s=5, timeout_s=600):\n",
    "    global _last_restart\n",
    "    deadline = time.time() + timeout_s\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            status = get_wf_status(wf_id).get(\"status\")\n",
    "            print(\"Status:\", status)\n",
    "            if status in (\"Succeeded\", \"Failed\", \"Aborted\"):\n",
    "                return status\n",
    "        except Exception:\n",
    "            now = time.time()\n",
    "            if now - _last_restart > 30:\n",
    "                print(\"Cromwell not reachable; restarting...\")\n",
    "                start_cromwell()\n",
    "                _last_restart = now\n",
    "        time.sleep(poll_s)\n",
    "    raise TimeoutError(f\"Workflow {wf_id} did not finish within {timeout_s}s\")\n",
    "\n",
    "def latest_workflow_id(wdl_name=None, status=None):\n",
    "    params = {\"page\": 1, \"pagesize\": 20}\n",
    "    if wdl_name:\n",
    "        params[\"name\"] = wdl_name\n",
    "    if status:\n",
    "        params[\"status\"] = status\n",
    "\n",
    "    r = requests.get(f\"{CROMWELL_API}/query\", params=params)\n",
    "    if r.status_code != 200:\n",
    "        payload = {\"page\": 1, \"pagesize\": 20}\n",
    "        if wdl_name:\n",
    "            payload[\"name\"] = wdl_name\n",
    "        if status:\n",
    "            payload[\"status\"] = status\n",
    "        r = requests.post(f\"{CROMWELL_API}/query\", json=payload)\n",
    "\n",
    "    r.raise_for_status()\n",
    "    results = r.json().get(\"results\", [])\n",
    "    if not results:\n",
    "        return None\n",
    "    results.sort(key=lambda x: x.get(\"submission\", \"\"), reverse=True)\n",
    "    return results[0].get(\"id\")\n",
    "\n",
    "def get_callroots(wf_id):\n",
    "    meta = get_wf_metadata(wf_id, include_keys=[\"callRoot\", \"calls\"])\n",
    "    callroots = []\n",
    "    calls = meta.get(\"calls\", {})\n",
    "    for call_name, entries in calls.items():\n",
    "        for e in entries:\n",
    "            if \"callRoot\" in e:\n",
    "                callroots.append((call_name, e[\"callRoot\"]))\n",
    "    return callroots\n",
    "\n",
    "def fetch_task_logs_from_gcs(wf_id, call_name=None):\n",
    "    callroots = get_callroots(wf_id)\n",
    "    if not callroots:\n",
    "        print(\"No callRoot entries found.\")\n",
    "        return\n",
    "\n",
    "    for name, root in callroots:\n",
    "        if call_name and call_name != name:\n",
    "            continue\n",
    "        stdout = f\"{root}/stdout\"\n",
    "        stderr = f\"{root}/stderr\"\n",
    "        print(f\"\\nCall: {name}\")\n",
    "        print(\"stdout:\", stdout)\n",
    "        print(\"stderr:\", stderr)\n",
    "        subprocess.run(f\"gsutil cat {stdout} | tail -n 50\", shell=True, check=False)\n",
    "        subprocess.run(f\"gsutil cat {stderr} | tail -n 50\", shell=True, check=False)\n",
    "\n",
    "def latest_workflow_id_gcs(workspace_bucket, workflow_name):\n",
    "    cmd = f\"gsutil ls -l {workspace_bucket}/workflows/cromwell-executions/{workflow_name}/\"\n",
    "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
    "    lines = [l for l in out.splitlines() if l.strip().startswith(\"gs://\")]\n",
    "    if not lines:\n",
    "        return None\n",
    "    lines.sort()\n",
    "    latest = lines[-1].split()[-1].rstrip(\"/\")\n",
    "    return latest.split(\"/\")[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Up Cromwell Server (Start up/ Validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cromwell_up():\", cromwell_up())\n",
    "print(\"cromwell_pid_running():\", cromwell_pid_running())\n",
    "print(\"cromwell_healthy():\", cromwell_healthy())\n",
    "\n",
    "start_cromwell()\n",
    "\n",
    "print(\"cromwell_up():\", cromwell_up())\n",
    "print(\"cromwell_pid_running():\", cromwell_pid_running())\n",
    "print(\"cromwell_healthy():\", cromwell_healthy())\n",
    "\n",
    "print(\"cromwell_persistent_ok():\", cromwell_persistent_ok())\n",
    "print(\"cromwell_persistent_recent():\", cromwell_persistent_recent())\n",
    "\n",
    "tail_logs(20)\n",
    "\n",
    "# Optional: latest workflow ID (if any exist)\n",
    "print(\"latest_workflow_id():\", latest_workflow_id())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"status endpoint:\", requests.get(CROMWELL_STATUS_URL).text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_logs(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cromwell Configuration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cromwell.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrades: add system tuning for throughput\n",
    "from pathlib import Path\n",
    "\n",
    "CROMWELL_DB = \"/home/jupyter/cromwell_db/local_cromwell_run.db\"\n",
    "Path(\"/home/jupyter/cromwell_db\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cromwell_conf = f\"\"\"include required(classpath(\"application\"))\n",
    "\n",
    "google {{\n",
    "  application-name = \"cromwell\"\n",
    "  auths = [{{\n",
    "    name = \"application_default\"\n",
    "    scheme = \"application_default\"\n",
    "  }}]\n",
    "}}\n",
    "\n",
    "system {{\n",
    "  new-workflow-poll-rate = 1\n",
    "  max-concurrent-workflows = 50\n",
    "  max-workflow-launch-count = 400\n",
    "  job-rate-control {{\n",
    "    jobs = 100\n",
    "    per = \"3 seconds\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "backend {{\n",
    "  default = \"GCPBATCH\"\n",
    "  providers {{\n",
    "    Local.config.root = \"/dev/null\"\n",
    "\n",
    "    GCPBATCH {{\n",
    "      actor-factory = \"cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory\"\n",
    "      config {{\n",
    "        project = \"{GOOGLE_PROJECT}\"\n",
    "        concurrent-job-limit = 20\n",
    "        root = \"{WORKSPACE_BUCKET}/workflows/cromwell-executions\"\n",
    "\n",
    "        virtual-private-cloud {{\n",
    "          network-name = \"projects/{GOOGLE_PROJECT}/global/networks/network\"\n",
    "          subnetwork-name = \"projects/{GOOGLE_PROJECT}/regions/us-central1/subnetworks/subnetwork\"\n",
    "        }}\n",
    "\n",
    "        batch {{\n",
    "          auth = \"application_default\"\n",
    "          compute-service-account = \"{PET_SA_EMAIL}\"\n",
    "          location = \"us-central1\"\n",
    "        }}\n",
    "\n",
    "        default-runtime-attributes {{\n",
    "          noAddress: true\n",
    "        }}\n",
    "\n",
    "        filesystems {{\n",
    "          gcs {{\n",
    "            auth = \"application_default\"\n",
    "          }}\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\n",
    "database {{\n",
    "  profile = \"slick.jdbc.HsqldbProfile$\"\n",
    "  insert-batch-size = 6000\n",
    "  db {{\n",
    "    driver = \"org.hsqldb.jdbcDriver\"\n",
    "    url = \"jdbc:hsqldb:file:{CROMWELL_DB};shutdown=false;hsqldb.default_table_type=cached;hsqldb.tx=mvcc;hsqldb.large_data=true;hsqldb.lob_compressed=true;hsqldb.script_format=3;hsqldb.result_max_memory_rows=20000\"\n",
    "    connectionTimeout = 300000\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "Path(\"/home/jupyter/cromwell.conf\").write_text(cromwell_conf)\n",
    "print(\"Wrote /home/jupyter/cromwell.conf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download or Generate Metadata "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7,
     35
    ]
   },
   "outputs": [],
   "source": [
    "# Download mtdna metadata \n",
    "# TODO: replace with call to RScript to regenerate this \n",
    "# Alternatively make \n",
    "import gzip\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_mtdna_tsv(download=False, print_head=False):\n",
    "    out_dir = Path(\"data/metadata\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/Kaychewe/mtDNA-analysis/meta/mtdna_mitoclock_aou_dataset_36246309_person_age_gender_crams.tsv.gz\"\n",
    "    path = out_dir / \"mtdna_mitoclock_aou_dataset_36246309_person_age_gender_crams.tsv.gz\"\n",
    "\n",
    "    if download or not path.exists():\n",
    "        subprocess.run([\"curl\", \"-L\", url, \"-o\", str(path)], check=True)\n",
    "        print(\"Downloaded:\", path.resolve())\n",
    "    else:\n",
    "        print(\"Using existing:\", path.resolve())\n",
    "\n",
    "    # gzip check + header\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        header = f.readline().strip()\n",
    "        first = f.readline().strip() if print_head else None\n",
    "\n",
    "    print(\"Gzip OK.\")\n",
    "    print(\"Header:\", header)\n",
    "    if print_head:\n",
    "        print(\"First row:\", first)\n",
    "\n",
    "    return path, header\n",
    "\n",
    "# Example usage:\n",
    "#ensure_mtdna_tsv(download=False, print_head=True)\n",
    "\n",
    "def load_and_sort_by_person_id(path, limit=None):\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        header = f.readline().strip().split(\"\\t\")\n",
    "        rows = [line.strip().split(\"\\t\") for line in f if line.strip()]\n",
    "\n",
    "    # find person_id column\n",
    "    pid_idx = header.index(\"person_id\")\n",
    "    rows.sort(key=lambda r: int(r[pid_idx]))\n",
    "\n",
    "    if limit:\n",
    "        rows = rows[:limit]\n",
    "\n",
    "    return header, rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expecting age, gender, sex, cram, cram_id \n",
    "path = ensure_mtdna_tsv(download=False, print_head=True)[0]\n",
    "header, rows = load_and_sort_by_person_id(path, limit=2)\n",
    "print(\"Header:\", header)\n",
    "print(\"First 10 rows:\")\n",
    "for r in rows:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = ensure_mtdna_tsv(download=False, print_head=True)[0]\n",
    "df = pd.read_csv(path, sep=\"\\t\", compression=\"gzip\")\n",
    "df = df.sort_values(\"person_id\").reset_index(drop=True)\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage01 (Comprehensive): CRAM → chrM + NUMT BAM/SAM\n",
    "\n",
    "**Goal**  \n",
    "Start from a WGS CRAM, subset to chrM + NUMT intervals, clean/standardize reads, and produce final BAM/BAI/SAM plus key QC.\n",
    "\n",
    "**Inputs**\n",
    "- CRAM + CRAI\n",
    "- Reference FASTA + index + dict\n",
    "- chrM interval list + NUMT interval list\n",
    "- sample metadata (age/sex) for naming\n",
    "\n",
    "**Outputs**\n",
    "- `out/<sample>_<age>_<sex>_chrM.proc.bam`\n",
    "- `out/<sample>_<age>_<sex>_chrM.proc.bam.bai`\n",
    "- `out/<sample>_<age>_<sex>_chrM.proc.sam`\n",
    "- `out/<sample>_<age>_<sex>_chrM.unmap.bam`\n",
    "- `out/<sample>_<age>_<sex>_chrM.duplicate.metrics`\n",
    "- `out/<sample>_<age>_<sex>_chrM.mean_coverage.txt`\n",
    "- `out/<sample>_<age>_<sex>_chrM.ct_failed.txt`\n",
    "\n",
    "**Key Steps**\n",
    "1. Subset CRAM to chrM + NUMT (`gatk PrintReads`)\n",
    "2. Validate + remove broken mates\n",
    "3. Remove malformed XQ tag (if present)\n",
    "4. Revert to unmapped\n",
    "5. Coverage metrics (mean coverage)\n",
    "6. Mark duplicates\n",
    "7. Sort + index\n",
    "8. Export SAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Stage01 Full GATK run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "wdl_text = \"\"\"\\\n",
    "version 1.0\n",
    "\n",
    "workflow stage01_SubsetCramChrM {\n",
    "  meta {\n",
    "    description: \"Stage01 (comprehensive): subset to chrM + NUMT, clean, mark duplicates, and emit BAM/BAI/SAM.\"\n",
    "  }\n",
    "\n",
    "  input {\n",
    "    File input_cram\n",
    "    File? input_crai\n",
    "    String sample_id\n",
    "    String? age\n",
    "    String? sex\n",
    "\n",
    "    File mt_interval_list\n",
    "    File numt_interval_list\n",
    "    File ref_fasta\n",
    "    File ref_fasta_index\n",
    "    File ref_dict\n",
    "\n",
    "    String docker\n",
    "\n",
    "    Int? mem_gb\n",
    "    Int? n_cpu\n",
    "    Int? preemptible_tries\n",
    "    String? requester_pays_project\n",
    "  }\n",
    "\n",
    "  call SubsetAndProcessChrM {\n",
    "    input:\n",
    "      input_cram = input_cram,\n",
    "      input_crai = input_crai,\n",
    "      sample_id = sample_id,\n",
    "      age = age,\n",
    "      sex = sex,\n",
    "      mt_interval_list = mt_interval_list,\n",
    "      numt_interval_list = numt_interval_list,\n",
    "      ref_fasta = ref_fasta,\n",
    "      ref_fasta_index = ref_fasta_index,\n",
    "      ref_dict = ref_dict,\n",
    "      docker = docker,\n",
    "      mem_gb = mem_gb,\n",
    "      n_cpu = n_cpu,\n",
    "      preemptible_tries = preemptible_tries,\n",
    "      requester_pays_project = requester_pays_project\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File final_bam = SubsetAndProcessChrM.final_bam\n",
    "    File final_bai = SubsetAndProcessChrM.final_bai\n",
    "    File final_sam = SubsetAndProcessChrM.final_sam\n",
    "    File unmapped_bam = SubsetAndProcessChrM.unmapped_bam\n",
    "    File duplicate_metrics = SubsetAndProcessChrM.duplicate_metrics\n",
    "    Int reads_dropped = SubsetAndProcessChrM.reads_dropped\n",
    "    Int mean_coverage = SubsetAndProcessChrM.mean_coverage\n",
    "  }\n",
    "}\n",
    "\n",
    "task SubsetAndProcessChrM {\n",
    "  input {\n",
    "    File input_cram\n",
    "    File? input_crai\n",
    "    String sample_id\n",
    "    String? age\n",
    "    String? sex\n",
    "\n",
    "    File mt_interval_list\n",
    "    File numt_interval_list\n",
    "    File ref_fasta\n",
    "    File ref_fasta_index\n",
    "    File ref_dict\n",
    "\n",
    "    String docker\n",
    "\n",
    "    Int? mem_gb\n",
    "    Int? n_cpu\n",
    "    Int? preemptible_tries\n",
    "    String? requester_pays_project\n",
    "  }\n",
    "\n",
    "  String age_label = select_first([age, \"NA\"])\n",
    "  String sex_label = select_first([sex, \"NA\"])\n",
    "  String prefix = sample_id + \"_\" + age_label + \"_\" + sex_label + \"_chrM\"\n",
    "\n",
    "  Float ref_size = size(ref_fasta, \"GB\") + size(ref_fasta_index, \"GB\") + size(ref_dict, \"GB\")\n",
    "  Int disk_size = ceil(ref_size) + ceil(size(input_cram, \"GB\")) + 20\n",
    "  Int machine_mem = select_first([mem_gb, 8])\n",
    "  Int command_mem = (machine_mem * 1000) - 500\n",
    "  String appended_crai = input_cram + \".crai\"\n",
    "\n",
    "  String d = \"$\"\n",
    "\n",
    "  command <<<\n",
    "    set -euo pipefail\n",
    "\n",
    "    mkdir -p out\n",
    "\n",
    "    this_cram=\"~{input_cram}\"\n",
    "    this_crai=\"~{select_first([input_crai, appended_crai])}\"\n",
    "\n",
    "    echo \"STEP 1: Subset CRAM to chrM + NUMT (PrintReads)\"\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" PrintReads \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -L ~{mt_interval_list} \\\n",
    "      -L ~{numt_interval_list} \\\n",
    "      ~{\"--gcs-project-for-requester-pays \" + requester_pays_project} \\\n",
    "      -I ~{d}{this_cram} --read-index ~{d}{this_crai} \\\n",
    "      -O \"out/~{prefix}.bam\"\n",
    "\n",
    "    echo \"STEP 2: Validate + remove broken mates\"\n",
    "    set +e\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" ValidateSamFile \\\n",
    "      -INPUT \"out/~{prefix}.bam\" \\\n",
    "      -O output.txt \\\n",
    "      -M VERBOSE \\\n",
    "      -IGNORE_WARNINGS true \\\n",
    "      -MAX_OUTPUT 9999999\n",
    "    cat output.txt | \\\n",
    "      grep \"ERROR.*Mate not found for paired read\" | \\\n",
    "      sed -e \"s/ERROR::MATE_NOT_FOUND:Read name //g\" | \\\n",
    "      sed -e \"s/, Mate not found for paired read//g\" > read_list.txt\n",
    "    cat read_list.txt | wc -l | sed \"s/^ *//g\" > \"out/~{prefix}.ct_failed.txt\"\n",
    "    if [[ $(tr -d \"\\\\r\\\\n\" < read_list.txt|wc -c) -eq 0 ]]; then\n",
    "      cp \"out/~{prefix}.bam\" rescued.bam\n",
    "    else\n",
    "      gatk --java-options \"-Xmx~{command_mem}m\" FilterSamReads \\\n",
    "        -I \"out/~{prefix}.bam\" \\\n",
    "        -O rescued.bam \\\n",
    "        -READ_LIST_FILE read_list.txt \\\n",
    "        -FILTER excludeReadList\n",
    "    fi\n",
    "    set -e\n",
    "\n",
    "    echo \"STEP 2.5: Remove malformed XQ tag\"\n",
    "    samtools view -h rescued.bam \\\n",
    "      | sed 's/\\\\tXQ:i:[0-9]\\\\+//g' \\\n",
    "      | samtools view -b -o cleaned.bam\n",
    "\n",
    "    echo \"STEP 3: Revert to unmapped (cleaned)\"\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" RevertSam \\\n",
    "      -INPUT cleaned.bam \\\n",
    "      -OUTPUT_BY_READGROUP false \\\n",
    "      -OUTPUT \"out/~{prefix}.unmap.bam\" \\\n",
    "      -VALIDATION_STRINGENCY LENIENT \\\n",
    "      -ATTRIBUTE_TO_CLEAR FT \\\n",
    "      -ATTRIBUTE_TO_CLEAR CO \\\n",
    "      -ATTRIBUTE_TO_CLEAR XQ \\\n",
    "      -SORT_ORDER queryname \\\n",
    "      -RESTORE_ORIGINAL_QUALITIES false\n",
    "\n",
    "    echo \"STEP 4: Collect WGS metrics\"\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" CollectWgsMetrics \\\n",
    "      INPUT=\"out/~{prefix}.bam\" \\\n",
    "      INTERVALS=~{mt_interval_list} \\\n",
    "      VALIDATION_STRINGENCY=SILENT \\\n",
    "      REFERENCE_SEQUENCE=~{ref_fasta} \\\n",
    "      OUTPUT=\"out/~{prefix}.wgs_metrics.txt\" \\\n",
    "      USE_FAST_ALGORITHM=true \\\n",
    "      READ_LENGTH=151 \\\n",
    "      COVERAGE_CAP=100000 \\\n",
    "      INCLUDE_BQ_HISTOGRAM=true \\\n",
    "      THEORETICAL_SENSITIVITY_OUTPUT=\"out/~{prefix}.theoretical_sensitivity.txt\"\n",
    "\n",
    "    R --vanilla <<CODE\n",
    "      df = read.table(\"out/~{prefix}.wgs_metrics.txt\",skip=6,header=TRUE,stringsAsFactors=FALSE,sep='\\\\t',nrows=1)\n",
    "      write.table(floor(df[,\"MEAN_COVERAGE\"]), \"out/~{prefix}.mean_coverage.txt\", quote=F, col.names=F, row.names=F)\n",
    "    CODE\n",
    "\n",
    "    echo \"STEP 5: Mark duplicates\"\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" MarkDuplicates \\\n",
    "      INPUT=\"out/~{prefix}.bam\" \\\n",
    "      OUTPUT=md.bam \\\n",
    "      METRICS_FILE=\"out/~{prefix}.duplicate.metrics\" \\\n",
    "      VALIDATION_STRINGENCY=SILENT \\\n",
    "      OPTICAL_DUPLICATE_PIXEL_DISTANCE=2500 \\\n",
    "      ASSUME_SORT_ORDER=\"queryname\" \\\n",
    "      CLEAR_DT=\"false\" \\\n",
    "      ADD_PG_TAG_TO_READS=false\n",
    "\n",
    "    echo \"STEP 6: Sort + index\"\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" SortSam \\\n",
    "      INPUT=md.bam \\\n",
    "      OUTPUT=\"out/~{prefix}.proc.bam\" \\\n",
    "      SORT_ORDER=\"coordinate\" \\\n",
    "      CREATE_INDEX=true \\\n",
    "      MAX_RECORDS_IN_RAM=300000\n",
    "\n",
    "    echo \"STEP 7: Export SAM\"\n",
    "    samtools view -h \"out/~{prefix}.proc.bam\" > \"out/~{prefix}.proc.sam\"\n",
    "  >>>\n",
    "\n",
    "  runtime {\n",
    "    memory: machine_mem + \" GB\"\n",
    "    disks: \"local-disk \" + disk_size + \" HDD\"\n",
    "    docker: docker\n",
    "    preemptible: select_first([preemptible_tries, 5])\n",
    "    cpu: select_first([n_cpu, 1])\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File final_bam = \"out/~{prefix}.proc.bam\"\n",
    "    File final_bai = \"out/~{prefix}.proc.bai\"\n",
    "    File final_sam = \"out/~{prefix}.proc.sam\"\n",
    "    File unmapped_bam = \"out/~{prefix}.unmap.bam\"\n",
    "    File duplicate_metrics = \"out/~{prefix}.duplicate.metrics\"\n",
    "    Int reads_dropped = read_int(\"out/~{prefix}.ct_failed.txt\")\n",
    "    Int mean_coverage = read_int(\"out/~{prefix}.mean_coverage.txt\")\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "out_dir = Path(\"./WDL/s001\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "wdl_path = out_dir / \"stage01_SubsetCramChrM_v2.wdl\"\n",
    "wdl_path.write_text(wdl_text)\n",
    "\n",
    "print(f\"Wrote: {wdl_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /home/jupyter/workspaces/mtdnaheteroplasmyandaginganalysis/WDL/s001/stage01_SubsetCramChrM.wdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Select sample ----\n",
    "SELECT_PERSON_ID = 1000696 \n",
    "SELECT_ROW_INDEX = 0\n",
    "\n",
    "if SELECT_PERSON_ID is not None:\n",
    "    row = df.loc[df[\"person_id\"] == SELECT_PERSON_ID].iloc[0]\n",
    "else:\n",
    "    row = df.iloc[SELECT_ROW_INDEX]\n",
    "\n",
    "sample_id = str(row[\"person_id\"])\n",
    "age = str(row[\"age\"]) if \"age\" in row and not pd.isna(row[\"age\"]) else None\n",
    "sex = str(row[\"sex_at_birth\"]) if \"sex_at_birth\" in row and not pd.isna(row[\"sex_at_birth\"]) else None\n",
    "\n",
    "# ---- Reference paths ----\n",
    "mt_interval_list = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/chrM.hg38.interval_list\"\n",
    "numt_interval_list = \"gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/intervals/NUMTv3_all385.hg38.interval_list\"\n",
    "\n",
    "ref_fasta = \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\"\n",
    "ref_fasta_index = \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai\"\n",
    "ref_dict = \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict\"\n",
    "\n",
    "inputs = {\n",
    "    \"stage01_SubsetCramChrM.input_cram\": row[\"cram_uri\"],\n",
    "    \"stage01_SubsetCramChrM.input_crai\": row[\"cram_index_uri\"],\n",
    "    \"stage01_SubsetCramChrM.sample_id\": sample_id,\n",
    "    \"stage01_SubsetCramChrM.age\": age,\n",
    "    \"stage01_SubsetCramChrM.sex\": sex,\n",
    "    \"stage01_SubsetCramChrM.mt_interval_list\": mt_interval_list,\n",
    "    \"stage01_SubsetCramChrM.numt_interval_list\": numt_interval_list,\n",
    "    \"stage01_SubsetCramChrM.ref_fasta\": ref_fasta,\n",
    "    \"stage01_SubsetCramChrM.ref_fasta_index\": ref_fasta_index,\n",
    "    \"stage01_SubsetCramChrM.ref_dict\": ref_dict,\n",
    "    \"stage01_SubsetCramChrM.docker\": \"kchewe/mtdna-tools:0.1.0\",\n",
    "    \"stage01_SubsetCramChrM.requester_pays_project\": GOOGLE_PROJECT,\n",
    "    \"stage01_SubsetCramChrM.mem_gb\": 16,\n",
    "    \"stage01_SubsetCramChrM.n_cpu\": 4,\n",
    "}\n",
    "\n",
    "out_path = Path(\"./WDL/s001/stage01_SubsetCramChrM.inputs_v2.json\")\n",
    "out_path.write_text(json.dumps(inputs, indent=2) + \"\\n\")\n",
    "print(\"Wrote:\", out_path.resolve())\n",
    "print(\"Selected sample:\", sample_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/jupyter/workspaces/mtdnaheteroplasmyandaginganalysis/WDL/s001/stage01_SubsetCramChrM.inputs_v2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Stage01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# Submit stage01 (comprehensive)\n",
    "start_cromwell()\n",
    "\n",
    "wdl_path = Path(\"./WDL/s001/stage01_SubsetCramChrM_v2.wdl\")\n",
    "json_path = Path(\"./WDL/s001/stage01_SubsetCramChrM.inputs_v2.json\")\n",
    "\n",
    "# Validate WDL\n",
    "validate_cmd = (\n",
    "    \"bash -lc 'source /home/jupyter/.sdkman/bin/sdkman-init.sh \"\n",
    "    \"&& sdk use java 17.0.8-tem \"\n",
    "    f\"&& java -jar womtool-91.jar validate {wdl_path}'\"\n",
    ")\n",
    "print(\"$\", validate_cmd)\n",
    "subprocess.run(validate_cmd, shell=True, check=True)\n",
    "\n",
    "# Submit to Cromwell\n",
    "cromwell_url = \"http://localhost:8094/api/workflows/v1\"\n",
    "files = {\n",
    "    \"workflowSource\": wdl_path.open(\"rb\"),\n",
    "    \"workflowInputs\": json_path.open(\"rb\"),\n",
    "}\n",
    "print(\"Submitting to:\", cromwell_url)\n",
    "resp = requests.post(cromwell_url, files=files, headers={\"accept\": \"application/json\"})\n",
    "resp.raise_for_status()\n",
    "wf = resp.json()\n",
    "print(\"Response:\", wf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Stage 01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor + logs\n",
    "wf_id = wf.get(\"id\")\n",
    "print(wf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wf_id)\n",
    "status = wait_for_wf(wf_id, poll_s=30, timeout_s=7200)\n",
    "print(\"Final status:\", status)\n",
    "\n",
    "pretty(get_wf_metadata(wf_id, include_keys=[\"failures\", \"callRoot\"]))\n",
    "fetch_task_logs_from_gcs(wf_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage02 (Comprehensive): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration (WDL and JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "wdl_text = \"\"\"\\\n",
    "version 1.0\n",
    "\n",
    "workflow stage02_MtOnly {\n",
    "  meta {\n",
    "    description: \"Stage02 (full, single-file): align/call mt + nuc, contamination, haplochecker.\"\n",
    "  }\n",
    "\n",
    "  input {\n",
    "    File input_bam\n",
    "    File input_bai\n",
    "    String sample_id\n",
    "    String? age\n",
    "    String? sex\n",
    "\n",
    "    File ref_dict\n",
    "    File ref_fasta\n",
    "    File ref_fasta_index\n",
    "\n",
    "    File mt_dict\n",
    "    File mt_fasta\n",
    "    File mt_fasta_index\n",
    "    File blacklisted_sites\n",
    "    File blacklisted_sites_index\n",
    "\n",
    "    File nuc_interval_list\n",
    "    File mt_interval_list\n",
    "\n",
    "    Int mt_mean_coverage\n",
    "\n",
    "    Boolean use_haplotype_caller_nucdna = true\n",
    "    Int hc_dp_lower_bound = 10\n",
    "\n",
    "    String docker\n",
    "    File? gatk_override\n",
    "    String gatk_version = \"4.2.6.0\"\n",
    "    String? m2_extra_args\n",
    "    String? m2_filter_extra_args\n",
    "    Float? vaf_filter_threshold\n",
    "    Float? f_score_beta\n",
    "    Boolean compress_output_vcf = false\n",
    "    Float? verifyBamID\n",
    "\n",
    "    Int? max_read_length\n",
    "    File haplocheck_zip\n",
    "\n",
    "    Int? preemptible_tries\n",
    "    Int? n_cpu\n",
    "  }\n",
    "\n",
    "  String age_label = select_first([age, \"NA\"])\n",
    "  String sex_label = select_first([sex, \"NA\"])\n",
    "  String sample_label = sample_id + \"_\" + age_label + \"_\" + sex_label\n",
    "\n",
    "  if (use_haplotype_caller_nucdna) {\n",
    "    call MongoHC as CallNucHCIntegrated {\n",
    "      input:\n",
    "        input_bam = input_bam,\n",
    "        input_bai = input_bai,\n",
    "        sample_name = sample_label,\n",
    "        nuc_interval_list = nuc_interval_list,\n",
    "        ref_fasta = ref_fasta,\n",
    "        ref_fai = ref_fasta_index,\n",
    "        ref_dict = ref_dict,\n",
    "        suffix = \".nuc\",\n",
    "        compress = compress_output_vcf,\n",
    "        gatk_override = gatk_override,\n",
    "        gatk_docker_override = docker,\n",
    "        gatk_version = gatk_version,\n",
    "        hc_dp_lower_bound = hc_dp_lower_bound,\n",
    "        mem = 4,\n",
    "        preemptible_tries = preemptible_tries,\n",
    "        n_cpu = n_cpu\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if (!use_haplotype_caller_nucdna) {\n",
    "    call MongoNucM2 as CallNucM2Integrated {\n",
    "      input:\n",
    "        input_bam = input_bam,\n",
    "        input_bai = input_bai,\n",
    "        sample_name = sample_label,\n",
    "\n",
    "        ref_fasta = ref_fasta,\n",
    "        ref_fai = ref_fasta_index,\n",
    "        ref_dict = ref_dict,\n",
    "        suffix = \".nuc\",\n",
    "        mt_interval_list = nuc_interval_list,\n",
    "\n",
    "        m2_extra_args = select_first([m2_extra_args, \"\"]),\n",
    "\n",
    "        max_alt_allele_count = 4,\n",
    "        vaf_filter_threshold = 0.95,\n",
    "        verifyBamID = verifyBamID,\n",
    "        compress = compress_output_vcf,\n",
    "\n",
    "        gatk_override = gatk_override,\n",
    "        gatk_docker_override = docker,\n",
    "        gatk_version = gatk_version,\n",
    "        mem = 4,\n",
    "        preemptible_tries = preemptible_tries,\n",
    "        n_cpu = n_cpu\n",
    "    }\n",
    "  }\n",
    "\n",
    "  Int M2_mem = if mt_mean_coverage > 25000 then 14 else 7\n",
    "\n",
    "  call MongoRunM2InitialFilterSplit as CallMt {\n",
    "    input:\n",
    "      sample_name = sample_label,\n",
    "      input_bam = input_bam,\n",
    "      input_bai = input_bai,\n",
    "      verifyBamID = verifyBamID,\n",
    "      mt_interval_list = mt_interval_list,\n",
    "      ref_fasta = mt_fasta,\n",
    "      ref_fai = mt_fasta_index,\n",
    "      ref_dict = mt_dict,\n",
    "      suffix = \"\",\n",
    "      compress = compress_output_vcf,\n",
    "      m2_extra_filtering_args = select_first([m2_filter_extra_args, \"\"]) + \" --min-median-mapping-quality 0\",\n",
    "      max_alt_allele_count = 4,\n",
    "      vaf_filter_threshold = 0,\n",
    "      blacklisted_sites = blacklisted_sites,\n",
    "      blacklisted_sites_index = blacklisted_sites_index,\n",
    "      f_score_beta = f_score_beta,\n",
    "      gatk_override = gatk_override,\n",
    "      gatk_docker_override = docker,\n",
    "      gatk_version = gatk_version,\n",
    "      m2_extra_args = select_first([m2_extra_args, \"\"]),\n",
    "      mem = M2_mem,\n",
    "      preemptible_tries = preemptible_tries,\n",
    "      n_cpu = n_cpu\n",
    "  }\n",
    "\n",
    "  call GetContamination {\n",
    "    input:\n",
    "      input_vcf = CallMt.vcf_for_haplochecker,\n",
    "      sample_name = sample_label,\n",
    "      mean_coverage = mt_mean_coverage,\n",
    "      preemptible_tries = preemptible_tries,\n",
    "      haplochecker_docker = docker,\n",
    "      haplocheck_zip = haplocheck_zip\n",
    "  }\n",
    "\n",
    "  call MongoM2FilterContaminationSplit as FilterContamination {\n",
    "    input:\n",
    "      raw_vcf = CallMt.filtered_vcf,\n",
    "      raw_vcf_index = CallMt.filtered_vcf_idx,\n",
    "      raw_vcf_stats = CallMt.stats,\n",
    "      sample_name = sample_label,\n",
    "      hasContamination = GetContamination.hasContamination,\n",
    "      contamination_major = GetContamination.major_level,\n",
    "      contamination_minor = GetContamination.minor_level,\n",
    "      suffix = \"\",\n",
    "      run_contamination = true,\n",
    "      verifyBamID = verifyBamID,\n",
    "      ref_fasta = mt_fasta,\n",
    "      ref_fai = mt_fasta_index,\n",
    "      ref_dict = mt_dict,\n",
    "      compress = compress_output_vcf,\n",
    "      gatk_override = gatk_override,\n",
    "      gatk_docker_override = docker,\n",
    "      gatk_version = gatk_version,\n",
    "      m2_extra_filtering_args = select_first([m2_filter_extra_args, \"\"]) + \" --min-median-mapping-quality 0\",\n",
    "      max_alt_allele_count = 4,\n",
    "      vaf_filter_threshold = vaf_filter_threshold,\n",
    "      blacklisted_sites = blacklisted_sites,\n",
    "      blacklisted_sites_index = blacklisted_sites_index,\n",
    "      f_score_beta = f_score_beta,\n",
    "      preemptible_tries = preemptible_tries\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File out_vcf = FilterContamination.filtered_vcf\n",
    "    File out_vcf_index = FilterContamination.filtered_vcf_idx\n",
    "    File split_vcf = FilterContamination.split_vcf\n",
    "    File split_vcf_index = FilterContamination.split_vcf_index\n",
    "    File nuc_vcf = select_first([CallNucHCIntegrated.full_pass_vcf, CallNucM2Integrated.full_pass_vcf])\n",
    "    File nuc_vcf_index = select_first([CallNucHCIntegrated.full_pass_vcf_index, CallNucM2Integrated.full_pass_vcf_index])\n",
    "    File nuc_vcf_unfiltered = select_first([CallNucHCIntegrated.filtered_vcf, CallNucM2Integrated.filtered_vcf])\n",
    "    File split_nuc_vcf = select_first([CallNucHCIntegrated.split_vcf, CallNucM2Integrated.split_vcf])\n",
    "    File split_nuc_vcf_index = select_first([CallNucHCIntegrated.split_vcf_index, CallNucM2Integrated.split_vcf_index])\n",
    "    Int nuc_variants_pass = select_first([CallNucHCIntegrated.post_filt_vars, CallNucM2Integrated.post_filt_vars])\n",
    "    File input_vcf_for_haplochecker = CallMt.vcf_for_haplochecker\n",
    "    File contamination_metrics = GetContamination.contamination_file\n",
    "    String major_haplogroup = GetContamination.major_hg\n",
    "    Float contamination = FilterContamination.contamination\n",
    "    String hasContamination = GetContamination.hasContamination\n",
    "    Float contamination_major = GetContamination.major_level\n",
    "    Float contamination_minor = GetContamination.minor_level\n",
    "  }\n",
    "}\n",
    "\n",
    "task GetContamination {\n",
    "  input {\n",
    "    File input_vcf\n",
    "    String sample_name\n",
    "    Int mean_coverage\n",
    "    File haplocheck_zip\n",
    "    String haplochecker_docker\n",
    "    Int? preemptible_tries\n",
    "  }\n",
    "\n",
    "  Int disk_size = ceil(size(input_vcf, \"GB\")) + 20\n",
    "  String d = \"$\"\n",
    "\n",
    "  command <<<\n",
    "  set -e\n",
    "\n",
    "  mkdir out\n",
    "  this_basename=out/\"~{sample_name}\"\n",
    "  this_mean_cov=\"~{mean_coverage}\"\n",
    "  this_vcf=\"~{input_vcf}\"\n",
    "\n",
    "  this_vcf_nvar=$(cat \"~{d}{this_vcf}\" | grep ^chrM | wc -l | sed 's/^ *//g')\n",
    "  echo \"~{sample_name} has VCF with ~{d}{this_vcf_nvar} variants for contamination.\"\n",
    "\n",
    "  zip_path=\"~{haplocheck_zip}\"\n",
    "  jar xf \"${zip_path}\"\n",
    "  chmod +x haplocheck\n",
    "\n",
    "  ./haplocheck --out output \"~{d}{this_vcf}\"\n",
    "\n",
    "  if [ -s output ]; then\n",
    "    sed 's/\\\"//g' output > output-noquotes\n",
    "  else\n",
    "    : > output-noquotes\n",
    "  fi\n",
    "\n",
    "  if grep -q \"SampleID\" output-noquotes; then\n",
    "    awk -F \"\\t\" 'NR==1{print;next}{$1=\"~{sample_name}\";print}' output-noquotes > output-noquotes.fixed\n",
    "    mv output-noquotes.fixed output-noquotes\n",
    "  fi\n",
    "\n",
    "  cp 'output-noquotes' \"~{d}{this_basename}_output_noquotes\"\n",
    "\n",
    "  FORMAT_ERROR=\"Bad contamination file format\"\n",
    "  if grep -q \"SampleID\" output-noquotes; then\n",
    "    grep \"SampleID\" output-noquotes > headers\n",
    "    if [ `awk '{print $2}' headers` != \"Contamination\" ]; then echo $FORMAT_ERROR; fi\n",
    "    if [ `awk '{print $6}' headers` != \"HgMajor\" ]; then echo $FORMAT_ERROR; fi\n",
    "    if [ `awk '{print $8}' headers` != \"HgMinor\" ]; then echo $FORMAT_ERROR; fi\n",
    "    if [ `awk '{print $14}' headers` != \"MeanHetLevelMajor\" ]; then echo $FORMAT_ERROR; fi\n",
    "    if [ `awk '{print $15}' headers` != \"MeanHetLevelMinor\" ]; then echo $FORMAT_ERROR; fi\n",
    "  else\n",
    "    echo $FORMAT_ERROR\n",
    "  fi\n",
    "\n",
    "  if grep -q \"SampleID\" output-noquotes && grep -v \"SampleID\" output-noquotes | grep -q . && [ \"~{d}{this_mean_cov}\" -gt 0 ] && [ \"~{d}{this_vcf_nvar}\" -gt 0 ]; then\n",
    "    grep -v \"SampleID\" output-noquotes > output-data\n",
    "    awk -F \"\\t\" '{print $2}' output-data > \"~{d}{this_basename}.contamination.txt\"\n",
    "    awk -F \"\\t\" '{print $6}' output-data > \"~{d}{this_basename}.major_hg.txt\"\n",
    "    awk -F \"\\t\" '{print $8}' output-data > \"~{d}{this_basename}.minor_hg.txt\"\n",
    "    awk -F \"\\t\" '{print $14}' output-data > \"~{d}{this_basename}.mean_het_major.txt\"\n",
    "    awk -F \"\\t\" '{print $15}' output-data > \"~{d}{this_basename}.mean_het_minor.txt\"\n",
    "  else\n",
    "    echo \"NO\" > \"~{d}{this_basename}.contamination.txt\"\n",
    "    echo \"NONE\" > \"~{d}{this_basename}.major_hg.txt\"\n",
    "    echo \"NONE\" > \"~{d}{this_basename}.minor_hg.txt\"\n",
    "    echo \"0.000\" > \"~{d}{this_basename}.mean_het_major.txt\"\n",
    "    echo \"0.000\" > \"~{d}{this_basename}.mean_het_minor.txt\"\n",
    "  fi\n",
    "  >>>\n",
    "  runtime {\n",
    "    preemptible: select_first([preemptible_tries, 5])\n",
    "    memory: \"3 GB\"\n",
    "    disks: \"local-disk \" + disk_size + \" HDD\"\n",
    "    docker: haplochecker_docker\n",
    "  }\n",
    "  output {\n",
    "    File contamination_file = \"out/~{sample_name}_output_noquotes\"\n",
    "    String hasContamination = read_string(\"out/~{sample_name}.contamination.txt\")\n",
    "    String major_hg = read_string(\"out/~{sample_name}.major_hg.txt\")\n",
    "    String minor_hg = read_string(\"out/~{sample_name}.minor_hg.txt\")\n",
    "    Float major_level = read_float(\"out/~{sample_name}.mean_het_major.txt\")\n",
    "    Float minor_level = read_float(\"out/~{sample_name}.mean_het_minor.txt\")\n",
    "  }\n",
    "}\n",
    "\n",
    "task MongoHC {\n",
    "  input {\n",
    "    File ref_fasta\n",
    "    File ref_fai\n",
    "    File ref_dict\n",
    "    File input_bam\n",
    "    File input_bai\n",
    "\n",
    "    String sample_name\n",
    "    String suffix = \"\"\n",
    "\n",
    "    Int max_reads_per_alignment_start = 75\n",
    "    String? hc_extra_args\n",
    "    Boolean make_bamout = false\n",
    "\n",
    "    File? nuc_interval_list\n",
    "    File? force_call_vcf\n",
    "    File? force_call_vcf_index\n",
    "\n",
    "    Boolean compress\n",
    "    String gatk_version\n",
    "    File? gatk_override\n",
    "    String? gatk_docker_override\n",
    "    Float? contamination\n",
    "\n",
    "    Int hc_dp_lower_bound\n",
    "\n",
    "    Int mem\n",
    "    Int? preemptible_tries\n",
    "    Int? n_cpu\n",
    "  }\n",
    "\n",
    "  Int machine_mem = if defined(mem) then mem * 1000 else 3500\n",
    "  Int command_mem = machine_mem - 500\n",
    "\n",
    "  Float ref_size = size(ref_fasta, \"GB\") + size(ref_fai, \"GB\") + size(ref_dict, \"GB\")\n",
    "  Int disk_size = ceil((size(input_bam, \"GB\") * 2) + ref_size) + 22\n",
    "\n",
    "  String d = \"$\"\n",
    "\n",
    "  command <<<\n",
    "    set -e\n",
    "\n",
    "    mkdir out\n",
    "    this_sample=out/\"~{sample_name}\"\n",
    "    this_basename=\"~{d}{this_sample}\"\"~{suffix}\"\n",
    "    bamoutfile=\"~{d}{this_basename}.bamout.bam\"\n",
    "    touch \"~{d}{bamoutfile}\"\n",
    "\n",
    "    if [[ ~{make_bamout} == 'true' ]]; then bamoutstr=\"--bam-output ~{d}{this_basename}.bamout.bam\"; else bamoutstr=\"\"; fi\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" HaplotypeCaller \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -I ~{input_bam} \\\n",
    "      ~{\"-L \" + nuc_interval_list} \\\n",
    "      -O \"~{d}{this_basename}.raw.vcf\" \\\n",
    "      ~{hc_extra_args} \\\n",
    "      -contamination ~{default=\"0\" contamination} \\\n",
    "      ~{\"--genotype-filtered-alleles --alleles \" + force_call_vcf} \\\n",
    "      --max-reads-per-alignment-start ~{max_reads_per_alignment_start} \\\n",
    "      --max-mnp-distance 0 \\\n",
    "      --annotation StrandBiasBySample \\\n",
    "      -G StandardAnnotation -G StandardHCAnnotation \\\n",
    "      -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 ~{d}{bamoutstr}\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" SelectVariants -V \"~{d}{this_basename}.raw.vcf\" -select-type SNP -O snps.vcf\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" VariantFiltration -V snps.vcf \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -O snps_filtered.vcf \\\n",
    "      -filter \"QD < 2.0\" --filter-name \"QD2\" \\\n",
    "      -filter \"QUAL < 30.0\" --filter-name \"QUAL30\" \\\n",
    "      -filter \"SOR > 3.0\" --filter-name \"SOR3\" \\\n",
    "      -filter \"FS > 60.0\" --filter-name \"FS60\" \\\n",
    "      -filter \"MQ < 40.0\" --filter-name \"MQ40\" \\\n",
    "      -filter \"MQRankSum < -12.5\" --filter-name \"MQRankSum-12.5\" \\\n",
    "      -filter \"ReadPosRankSum < -8.0\" --filter-name \"ReadPosRankSum-8\" \\\n",
    "      --genotype-filter-expression \"isHet == 1\" --genotype-filter-name \"isHetFilt\" \\\n",
    "      --genotype-filter-expression \"isHomRef == 1\" --genotype-filter-name \"isHomRefFilt\" \\\n",
    "      ~{'--genotype-filter-expression \"DP < ' + hc_dp_lower_bound + '\" --genotype-filter-name \"genoDP' + hc_dp_lower_bound + '\"'}\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" SelectVariants -V \"~{d}{this_basename}.raw.vcf\" -select-type INDEL -O indels.vcf\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" VariantFiltration -V indels.vcf \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -O indels_filtered.vcf \\\n",
    "      -filter \"QD < 2.0\" --filter-name \"QD2\" \\\n",
    "      -filter \"QUAL < 30.0\" --filter-name \"QUAL30\" \\\n",
    "      -filter \"FS > 200.0\" --filter-name \"FS200\" \\\n",
    "      -filter \"SOR > 10.0\" --filter-name \"SOR10\" \\\n",
    "      -filter \"ReadPosRankSum < -20.0\" --filter-name \"ReadPosRankSum-20\" \\\n",
    "      --genotype-filter-expression \"isHet == 1\" --genotype-filter-name \"isHetFilt\" \\\n",
    "      --genotype-filter-expression \"isHomRef == 1\" --genotype-filter-name \"isHomRefFilt\" \\\n",
    "      ~{'--genotype-filter-expression \"DP < ' + hc_dp_lower_bound + '\" --genotype-filter-name \"genoDP' + hc_dp_lower_bound + '\"'}\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" MergeVcfs -I snps_filtered.vcf -I indels_filtered.vcf -O \"~{d}{this_basename}.vcf\"\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" SelectVariants \\\n",
    "      -V \"~{d}{this_basename}.vcf\" \\\n",
    "      --exclude-filtered \\\n",
    "      --set-filtered-gt-to-nocall \\\n",
    "      --exclude-non-variants \\\n",
    "      -O \"~{d}{this_basename}.pass.vcf\"\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" CountVariants -V $this_basename.pass.vcf | tail -n1 > \"~{d}{this_basename}.passvars.txt\"\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" LeftAlignAndTrimVariants \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -V \"~{d}{this_basename}.pass.vcf\" \\\n",
    "      -O \"~{d}{this_basename}.pass.split.vcf\" \\\n",
    "      --split-multi-allelics \\\n",
    "      --dont-trim-alleles \\\n",
    "      --keep-original-ac \\\n",
    "      --create-output-variant-index\n",
    "  >>>\n",
    "\n",
    "  runtime {\n",
    "    docker: select_first([gatk_docker_override, \"us.gcr.io/broad-gatk/gatk:\"+gatk_version])\n",
    "    memory: machine_mem + \" MB\"\n",
    "    disks: \"local-disk \" + disk_size + \" HDD\"\n",
    "    preemptible: select_first([preemptible_tries, 5])\n",
    "    cpu: select_first([n_cpu,1])\n",
    "  }\n",
    "  output {\n",
    "    File raw_vcf = \"out/~{sample_name}~{suffix}.raw.vcf\"\n",
    "    File raw_vcf_idx = \"out/~{sample_name}~{suffix}.raw.vcf.idx\"\n",
    "    File output_bamOut = \"out/~{sample_name}~{suffix}.bamout.bam\"\n",
    "    File filtered_vcf = \"out/~{sample_name}~{suffix}.vcf\"\n",
    "    File filtered_vcf_idx = \"out/~{sample_name}~{suffix}.vcf.idx\"\n",
    "    File full_pass_vcf = \"out/~{sample_name}~{suffix}.pass.vcf\"\n",
    "    File full_pass_vcf_index = \"out/~{sample_name}~{suffix}.pass.vcf.idx\"\n",
    "    Int post_filt_vars = read_int(\"out/~{sample_name}~{suffix}.passvars.txt\")\n",
    "    File split_vcf = \"out/~{sample_name}~{suffix}.pass.split.vcf\"\n",
    "    File split_vcf_index = \"out/~{sample_name}~{suffix}.pass.split.vcf.idx\"\n",
    "  }\n",
    "}\n",
    "\n",
    "task MongoNucM2 {\n",
    "  input {\n",
    "    File ref_fasta\n",
    "    File ref_fai\n",
    "    File ref_dict\n",
    "    File input_bam\n",
    "    File input_bai\n",
    "\n",
    "    String sample_name\n",
    "    String suffix = \"\"\n",
    "\n",
    "    Int max_reads_per_alignment_start = 75\n",
    "    String? m2_extra_args\n",
    "    Boolean make_bamout = false\n",
    "    Boolean compress\n",
    "\n",
    "    File? mt_interval_list\n",
    "\n",
    "    Float? vaf_cutoff\n",
    "    String? m2_extra_filtering_args\n",
    "    Int max_alt_allele_count\n",
    "    Float? vaf_filter_threshold\n",
    "    Float? f_score_beta\n",
    "    Float? verifyBamID\n",
    "    File? blacklisted_sites\n",
    "    File? blacklisted_sites_index\n",
    "\n",
    "    File? gatk_override\n",
    "    String gatk_version\n",
    "    String? gatk_docker_override\n",
    "    Int mem\n",
    "    Int? preemptible_tries\n",
    "    Int? n_cpu\n",
    "  }\n",
    "\n",
    "  Float ref_size = size(ref_fasta, \"GB\") + size(ref_fai, \"GB\")\n",
    "  Int disk_size = ceil(size(input_bam, \"GB\")*2 + ref_size) + 20\n",
    "  Float defval = 0.0\n",
    "\n",
    "  Int machine_mem = if defined(mem) then mem * 1000 else 3500\n",
    "  Int command_mem = machine_mem - 500\n",
    "\n",
    "  String d = \"$\"\n",
    "\n",
    "  command <<<\n",
    "    set -e\n",
    "\n",
    "    mkdir out\n",
    "    this_sample=out/\"~{sample_name}\"\n",
    "    this_contamination=\"~{select_first([verifyBamID, defval])}\"\n",
    "    this_bam=\"~{input_bam}\"\n",
    "    this_basename=\"~{d}{this_sample}~{suffix}\"\n",
    "    bamoutfile=\"~{d}{this_basename}.bamout.bam\"\n",
    "    touch \"~{d}{bamoutfile}\"\n",
    "    if [[ ~{make_bamout} == 'true' ]]; then bamoutstr=\"--bam-output ~{d}{bamoutfile}\"; else bamoutstr=\"\"; fi\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" Mutect2 \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -I \"~{d}{this_bam}\" \\\n",
    "      ~{\"-L \" + mt_interval_list} \\\n",
    "      -O \"~{d}{this_basename}.raw.vcf\" \\\n",
    "      ~{m2_extra_args} \\\n",
    "      ~{\"--minimum-allele-fraction \" + vaf_filter_threshold} \\\n",
    "      --annotation StrandBiasBySample \\\n",
    "      --max-reads-per-alignment-start ~{max_reads_per_alignment_start} \\\n",
    "      --max-mnp-distance 0 ~{d}{bamoutstr}\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" FilterMutectCalls -V \"~{d}{this_basename}.raw.vcf\" \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -O filtered.vcf \\\n",
    "      --stats \"~{d}{this_basename}.raw.vcf.stats\" \\\n",
    "      ~{m2_extra_filtering_args} \\\n",
    "      --max-alt-allele-count ~{max_alt_allele_count} \\\n",
    "      ~{\"--min-allele-fraction \" + vaf_filter_threshold} \\\n",
    "      ~{\"--f-score-beta \" + f_score_beta} \\\n",
    "      --contamination-estimate \"~{d}{this_contamination}\"\n",
    "\n",
    "    ~{\"gatk IndexFeatureFile -I \" + blacklisted_sites}\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" VariantFiltration -V filtered.vcf \\\n",
    "      -O \"~{d}{this_basename}.vcf\" \\\n",
    "      --apply-allele-specific-filters \\\n",
    "      ~{\"--mask-name 'blacklisted_site' --mask \" + blacklisted_sites}\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" SelectVariants \\\n",
    "      -V \"~{d}{this_basename}.vcf\" \\\n",
    "      --exclude-filtered \\\n",
    "      -O \"~{d}{this_basename}.pass.vcf\"\n",
    "\n",
    "    gatk CountVariants -V \"~{d}{this_basename}.pass.vcf\" | tail -n1 > \"~{d}{this_basename}.passvars.txt\"\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" LeftAlignAndTrimVariants \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -V \"~{d}{this_basename}.pass.vcf\" \\\n",
    "      -O \"~{d}{this_basename}.pass.split.vcf\" \\\n",
    "      --split-multi-allelics \\\n",
    "      --dont-trim-alleles \\\n",
    "      --keep-original-ac \\\n",
    "      --create-output-variant-index\n",
    "  >>>\n",
    "  runtime {\n",
    "    docker: select_first([gatk_docker_override, \"us.gcr.io/broad-gatk/gatk:\"+gatk_version])\n",
    "    memory: machine_mem + \" MB\"\n",
    "    disks: \"local-disk \" + disk_size + \" HDD\"\n",
    "    preemptible: select_first([preemptible_tries, 5])\n",
    "    cpu: select_first([n_cpu,2])\n",
    "  }\n",
    "  output {\n",
    "    File raw_vcf = \"out/~{sample_name}~{suffix}.raw.vcf\"\n",
    "    File raw_vcf_idx = \"out/~{sample_name}~{suffix}.raw.vcf.idx\"\n",
    "    File stats = \"out/~{sample_name}~{suffix}.raw.vcf.stats\"\n",
    "    File output_bamOut = \"out/~{sample_name}~{suffix}.bamout.bam\"\n",
    "\n",
    "    File filtered_vcf = \"out/~{sample_name}~{suffix}.vcf\"\n",
    "    File filtered_vcf_idx = \"out/~{sample_name}~{suffix}.vcf.idx\"\n",
    "\n",
    "    File full_pass_vcf = \"out/~{sample_name}~{suffix}.pass.vcf\"\n",
    "    File full_pass_vcf_index = \"out/~{sample_name}~{suffix}.pass.vcf.idx\"\n",
    "    Int post_filt_vars = read_int(\"out/~{sample_name}~{suffix}.passvars.txt\")\n",
    "\n",
    "    File split_vcf = \"out/~{sample_name}~{suffix}.pass.split.vcf\"\n",
    "    File split_vcf_index = \"out/~{sample_name}~{suffix}.pass.split.vcf.idx\"\n",
    "  }\n",
    "}\n",
    "\n",
    "task MongoRunM2InitialFilterSplit {\n",
    "  input {\n",
    "    String sample_name\n",
    "    File input_bam\n",
    "    File input_bai\n",
    "    Float? verifyBamID\n",
    "    String suffix\n",
    "\n",
    "    File ref_fasta\n",
    "    File ref_fai\n",
    "    File ref_dict\n",
    "    Int max_reads_per_alignment_start = 75\n",
    "    String? m2_extra_args\n",
    "    Boolean make_bamout = false\n",
    "    Boolean compress\n",
    "\n",
    "    File? mt_interval_list\n",
    "\n",
    "    Float? vaf_cutoff\n",
    "    String? m2_extra_filtering_args\n",
    "    Int max_alt_allele_count\n",
    "    Float? vaf_filter_threshold\n",
    "    Float? f_score_beta\n",
    "\n",
    "    File? blacklisted_sites\n",
    "    File? blacklisted_sites_index\n",
    "\n",
    "    String? gatk_docker_override\n",
    "    File? gatk_override\n",
    "    String gatk_version\n",
    "    Int mem\n",
    "    Int? preemptible_tries\n",
    "    Int? n_cpu\n",
    "  }\n",
    "\n",
    "  Float ref_size = size(ref_fasta, \"GB\") + size(ref_fai, \"GB\")\n",
    "  Int disk_size = (ceil(size(input_bam, \"GB\") + ref_size) * 2) + 20\n",
    "  Float defval = 0.0\n",
    "\n",
    "  Int machine_mem = if defined(mem) then mem * 1000 else 3500\n",
    "  Int command_mem = machine_mem - 500\n",
    "\n",
    "  String d = \"$\"\n",
    "\n",
    "  command <<<\n",
    "    set -e\n",
    "\n",
    "    mkdir out\n",
    "    this_sample=out/\"~{sample_name}\"\n",
    "    this_contamination=\"~{select_first([verifyBamID, defval])}\"\n",
    "    this_basename=\"~{d}{this_sample}~{suffix}\"\n",
    "    bamoutfile=\"~{d}{this_basename}.bamout.bam\"\n",
    "    touch \"~{d}{bamoutfile}\"\n",
    "    if [[ ~{make_bamout} == 'true' ]]; then bamoutstr=\"--bam-output ~{d}{bamoutfile}\"; else bamoutstr=\"\"; fi\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" Mutect2 \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -I ~{input_bam} \\\n",
    "      ~{\"-L \" + mt_interval_list} \\\n",
    "      -O \"~{d}{this_basename}.raw.vcf\" \\\n",
    "      ~{m2_extra_args} \\\n",
    "      --annotation StrandBiasBySample \\\n",
    "      --read-filter MateOnSameContigOrNoMappedMateReadFilter \\\n",
    "      --read-filter MateUnmappedAndUnmappedReadFilter \\\n",
    "      --mitochondria-mode \\\n",
    "      --max-reads-per-alignment-start ~{max_reads_per_alignment_start} \\\n",
    "      --max-mnp-distance 0 ~{d}{bamoutstr}\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" FilterMutectCalls -V \"~{d}{this_basename}.raw.vcf\" \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -O filtered.vcf \\\n",
    "      --stats \"~{d}{this_basename}.raw.vcf.stats\" \\\n",
    "      ~{m2_extra_filtering_args} \\\n",
    "      --max-alt-allele-count ~{max_alt_allele_count} \\\n",
    "      --mitochondria-mode \\\n",
    "      ~{\"--min-allele-fraction \" + vaf_filter_threshold} \\\n",
    "      ~{\"--f-score-beta \" + f_score_beta} \\\n",
    "      --contamination-estimate \"~{d}{this_contamination}\"\n",
    "\n",
    "    ~{\"gatk IndexFeatureFile -I \" + blacklisted_sites}\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" VariantFiltration -V filtered.vcf \\\n",
    "      -O \"~{d}{this_basename}.filtered.vcf\" \\\n",
    "      --apply-allele-specific-filters \\\n",
    "      ~{\"--mask-name 'blacklisted_site' --mask \" + blacklisted_sites}\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" LeftAlignAndTrimVariants \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -V \"~{d}{this_basename}.filtered.vcf\" \\\n",
    "      -O split.vcf \\\n",
    "      --split-multi-allelics \\\n",
    "      --dont-trim-alleles \\\n",
    "      --keep-original-ac\n",
    "\n",
    "    gatk --java-options \"-Xmx~{command_mem}m\" SelectVariants \\\n",
    "      -V split.vcf \\\n",
    "      -O \"~{d}{this_basename}.splitAndPassOnly.vcf\" \\\n",
    "      --exclude-filtered\n",
    "  >>>\n",
    "  runtime {\n",
    "    docker: select_first([gatk_docker_override, \"us.gcr.io/broad-gatk/gatk:\"+gatk_version])\n",
    "    memory: machine_mem + \" MB\"\n",
    "    disks: \"local-disk \" + disk_size + \" HDD\"\n",
    "    preemptible: select_first([preemptible_tries, 5])\n",
    "    cpu: select_first([n_cpu,2])\n",
    "  }\n",
    "  output {\n",
    "    File raw_vcf = \"out/~{sample_name}~{suffix}.raw.vcf\"\n",
    "    File raw_vcf_idx = \"out/~{sample_name}~{suffix}.raw.vcf.idx\"\n",
    "    File stats = \"out/~{sample_name}~{suffix}.raw.vcf.stats\"\n",
    "    File output_bamOut = \"out/~{sample_name}~{suffix}.bamout.bam\"\n",
    "\n",
    "    File filtered_vcf = \"out/~{sample_name}~{suffix}.filtered.vcf\"\n",
    "    File filtered_vcf_idx = \"out/~{sample_name}~{suffix}.filtered.vcf.idx\"\n",
    "\n",
    "    File vcf_for_haplochecker = \"out/~{sample_name}~{suffix}.splitAndPassOnly.vcf\"\n",
    "  }\n",
    "}\n",
    "\n",
    "task MongoM2FilterContaminationSplit {\n",
    "  input {\n",
    "    File raw_vcf\n",
    "    File raw_vcf_index\n",
    "    File raw_vcf_stats\n",
    "    String sample_name\n",
    "    String hasContamination\n",
    "    Float contamination_major\n",
    "    Float contamination_minor\n",
    "    Float? verifyBamID\n",
    "\n",
    "    Boolean run_contamination\n",
    "    File ref_fasta\n",
    "    File ref_fai\n",
    "    File ref_dict\n",
    "\n",
    "    Boolean compress\n",
    "    Float? vaf_cutoff\n",
    "    String suffix\n",
    "\n",
    "    String? m2_extra_filtering_args\n",
    "    Int max_alt_allele_count\n",
    "    Float? vaf_filter_threshold\n",
    "    Float? f_score_beta\n",
    "\n",
    "    File? blacklisted_sites\n",
    "    File? blacklisted_sites_index\n",
    "\n",
    "    File? gatk_override\n",
    "    String? gatk_docker_override\n",
    "    String gatk_version\n",
    "\n",
    "    Int? preemptible_tries\n",
    "  }\n",
    "\n",
    "  Float ref_size = size(ref_fasta, \"GB\") + size(ref_fai, \"GB\")\n",
    "  Int disk_size = ceil(size(raw_vcf, \"GB\") + ref_size) + 20\n",
    "  Float defval = 0.0\n",
    "  String d = \"$\"\n",
    "\n",
    "  command <<<\n",
    "    set -e\n",
    "\n",
    "    mkdir out\n",
    "\n",
    "    this_sample=out/\"~{sample_name}\"\n",
    "    this_raw_vcf=\"~{raw_vcf}\"\n",
    "    this_raw_stats=\"~{raw_vcf_stats}\"\n",
    "    this_has_contam=\"~{hasContamination}\"\n",
    "    this_verifybam=\"~{select_first([verifyBamID, defval])}\"\n",
    "    this_contam_major=\"~{contamination_major}\"\n",
    "    this_contam_minor=\"~{contamination_minor}\"\n",
    "\n",
    "    this_basename=\"~{d}{this_sample}~{suffix}\"\n",
    "    bamoutfile=\"~{d}{this_basename}.bamout.bam\"\n",
    "    touch \"~{d}{bamoutfile}\"\n",
    "\n",
    "    if [[ \"~{d}{this_has_contam}\" == 'YES' ]]; then\n",
    "      if (( $(echo \"~{d}{this_contam_major} == 0.0\"|bc -l) )); then\n",
    "        this_hc_contamination=\"~{d}{this_contam_minor}\"\n",
    "      else\n",
    "        this_hc_contamination=$( bc <<< \"1-~{d}{this_contam_major}\" )\n",
    "      fi\n",
    "    else\n",
    "      this_hc_contamination=0.0\n",
    "    fi\n",
    "\n",
    "    echo \"~{d}{this_hc_contamination}\" > \"~{d}{this_basename}.hc_contam.txt\"\n",
    "\n",
    "    if (( $(echo \"~{d}{this_verifybam} > ~{d}{this_hc_contamination}\"|bc -l) )); then\n",
    "      this_max_contamination=\"~{d}{this_verifybam}\"\n",
    "    else\n",
    "      this_max_contamination=\"~{d}{this_hc_contamination}\"\n",
    "    fi\n",
    "\n",
    "    gatk --java-options \"-Xmx2500m\" FilterMutectCalls \\\n",
    "      -V \"~{d}{this_raw_vcf}\" \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -O filtered.vcf \\\n",
    "      --stats \"~{d}{this_raw_stats}\" \\\n",
    "      ~{m2_extra_filtering_args} \\\n",
    "      --max-alt-allele-count ~{max_alt_allele_count} \\\n",
    "      --mitochondria-mode \\\n",
    "      ~{\"--min-allele-fraction \" + vaf_filter_threshold} \\\n",
    "      ~{\"--f-score-beta \" + f_score_beta} \\\n",
    "      --contamination-estimate \"~{d}{this_max_contamination}\"\n",
    "\n",
    "    ~{\"gatk IndexFeatureFile -I \" + blacklisted_sites}\n",
    "\n",
    "    gatk --java-options \"-Xmx2500m\" VariantFiltration \\\n",
    "      -V filtered.vcf \\\n",
    "      -O \"~{d}{this_basename}.vcf\" \\\n",
    "      --apply-allele-specific-filters \\\n",
    "      ~{\"--mask-name 'blacklisted_site' --mask \" + blacklisted_sites}\n",
    "\n",
    "    gatk --java-options \"-Xmx2500m\" LeftAlignAndTrimVariants \\\n",
    "      -R ~{ref_fasta} \\\n",
    "      -V \"~{d}{this_basename}.vcf\" \\\n",
    "      -O \"~{d}{this_basename}.split.vcf\" \\\n",
    "      --split-multi-allelics \\\n",
    "      --dont-trim-alleles \\\n",
    "      --keep-original-ac \\\n",
    "      --create-output-variant-index\n",
    "  >>>\n",
    "  runtime {\n",
    "    docker: select_first([gatk_docker_override, \"us.gcr.io/broad-gatk/gatk:\"+gatk_version])\n",
    "    memory: \"4 MB\"\n",
    "    disks: \"local-disk \" + disk_size + \" HDD\"\n",
    "    preemptible: select_first([preemptible_tries, 5])\n",
    "    cpu: 2\n",
    "  }\n",
    "  output {\n",
    "    File filtered_vcf = \"out/~{sample_name}~{suffix}.vcf\"\n",
    "    File filtered_vcf_idx = \"out/~{sample_name}~{suffix}.vcf.idx\"\n",
    "    File split_vcf = \"out/~{sample_name}~{suffix}.split.vcf\"\n",
    "    File split_vcf_index = \"out/~{sample_name}~{suffix}.split.vcf.idx\"\n",
    "    Float contamination = read_float(\"out/~{sample_name}~{suffix}.hc_contam.txt\")\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "out_dir = Path(\"./WDL/s002\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "wdl_path = out_dir / \"stage02_MtOnly_v2.wdl\"\n",
    "wdl_path.write_text(wdl_text)\n",
    "\n",
    "print(f\"Wrote: {wdl_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /home/jupyter/workspaces/mtdnaheteroplasmyandaginganalysis/WDL/s002/stage02_MtOnly_v2.wdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- choose stage01 workflow id (latest) ----\n",
    "WF_ID = latest_workflow_id(\"stage01_SubsetCramChrM\")\n",
    "print(\"Stage01 WF_ID:\", WF_ID)\n",
    "\n",
    "# ---- pull outputs from Cromwell metadata ----\n",
    "meta = get_wf_metadata(WF_ID, include_keys=[\"outputs\"])\n",
    "outputs = meta.get(\"outputs\", {})\n",
    "\n",
    "input_bam = outputs.get(\"stage01_SubsetCramChrM.final_bam\")\n",
    "input_bai = outputs.get(\"stage01_SubsetCramChrM.final_bai\")\n",
    "sample_label = outputs.get(\"stage01_SubsetCramChrM.final_bam\", \"\").split(\"/\")[-1].replace(\".proc.bam\", \"\")\n",
    "\n",
    "if not input_bam or not input_bai:\n",
    "    raise ValueError(\"Could not find Stage01 outputs in metadata.\")\n",
    "\n",
    "# parse sample_id / age / sex from filename\n",
    "# format: <sample_id>_<age>_<sex>_chrM.proc.bam\n",
    "parts = sample_label.split(\"_\")\n",
    "sample_id = parts[0] if len(parts) > 0 else \"\"\n",
    "age = parts[1] if len(parts) > 1 else None\n",
    "sex = parts[2] if len(parts) > 2 else None\n",
    "\n",
    "# ---- mt reference paths ----\n",
    "mt_fasta = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.fasta\"\n",
    "mt_fasta_index = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.fasta.fai\"\n",
    "mt_dict = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.dict\"\n",
    "mt_interval_list = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/chrM.hg38.interval_list\"\n",
    "\n",
    "# ---- nuc + blacklist ----\n",
    "ref_fasta = \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\"\n",
    "ref_fasta_index = \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai\"\n",
    "ref_dict = \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dict\"\n",
    "nuc_interval_list = \"gs://gcp-public-data--broad-references/hg38/v0/wgs_calling_regions.hg38.interval_list\"\n",
    "\n",
    "blacklisted_sites = \"gs://gcp-public-data--broad-references/hg38/v0/mitochondria/blacklisted_sites.vcf\"\n",
    "blacklisted_sites_index = \"gs://gcp-public-data--broad-references/hg38/v0/mitochondria/blacklisted_sites.vcf.idx\"\n",
    "\n",
    "# ---- haplochecker zip (from mtSwirl or your workspace) ----\n",
    "haplocheck_zip = \"gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/refs/haplocheck.zip\"\n",
    "\n",
    "# ---- build inputs ----\n",
    "inputs = {\n",
    "    \"stage02_MtOnly.input_bam\": input_bam,\n",
    "    \"stage02_MtOnly.input_bai\": input_bai,\n",
    "    \"stage02_MtOnly.sample_id\": sample_id,\n",
    "    \"stage02_MtOnly.age\": age,\n",
    "    \"stage02_MtOnly.sex\": sex,\n",
    "\n",
    "    \"stage02_MtOnly.ref_dict\": ref_dict,\n",
    "    \"stage02_MtOnly.ref_fasta\": ref_fasta,\n",
    "    \"stage02_MtOnly.ref_fasta_index\": ref_fasta_index,\n",
    "\n",
    "    \"stage02_MtOnly.mt_dict\": mt_dict,\n",
    "    \"stage02_MtOnly.mt_fasta\": mt_fasta,\n",
    "    \"stage02_MtOnly.mt_fasta_index\": mt_fasta_index,\n",
    "    \"stage02_MtOnly.blacklisted_sites\": blacklisted_sites,\n",
    "    \"stage02_MtOnly.blacklisted_sites_index\": blacklisted_sites_index,\n",
    "\n",
    "    \"stage02_MtOnly.nuc_interval_list\": nuc_interval_list,\n",
    "    \"stage02_MtOnly.mt_interval_list\": mt_interval_list,\n",
    "\n",
    "    \"stage02_MtOnly.mt_mean_coverage\": outputs.get(\"stage01_SubsetCramChrM.mean_coverage\", 0),\n",
    "\n",
    "    \"stage02_MtOnly.docker\": \"kchewe/mtdna-tools:0.1.0\",\n",
    "    \"stage02_MtOnly.haplocheck_zip\": haplocheck_zip,\n",
    "\n",
    "    \"stage02_MtOnly.use_haplotype_caller_nucdna\": True,\n",
    "    \"stage02_MtOnly.hc_dp_lower_bound\": 10,\n",
    "    \"stage02_MtOnly.vaf_filter_threshold\": 0.01,\n",
    "    \"stage02_MtOnly.f_score_beta\": 1.0,\n",
    "    \"stage02_MtOnly.compress_output_vcf\": False,\n",
    "    \"stage02_MtOnly.n_cpu\": 2,\n",
    "    \"stage02_MtOnly.preemptible_tries\": 5,\n",
    "}\n",
    "\n",
    "out_path = Path(\"./WDL/s002/stage02_MtOnly.inputs_v2.json\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_path.write_text(json.dumps(inputs, indent=2) + \"\\n\")\n",
    "print(\"Wrote:\", out_path.resolve())\n",
    "print(\"sample_id:\", sample_id, \"age:\", age, \"sex:\", sex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor Stage02 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "start_cromwell()\n",
    "\n",
    "wdl_path = Path(\"./WDL/s002/stage02_MtOnly_v2.wdl\")\n",
    "json_path = Path(\"./WDL/s002/stage02_MtOnly.inputs_v2.json\")\n",
    "\n",
    "# Validate WDL\n",
    "validate_cmd = (\n",
    "    \"bash -lc 'source /home/jupyter/.sdkman/bin/sdkman-init.sh \"\n",
    "    \"&& sdk use java 17.0.8-tem \"\n",
    "    f\"&& java -jar womtool-91.jar validate {wdl_path}'\"\n",
    ")\n",
    "print(\"$\", validate_cmd)\n",
    "subprocess.run(validate_cmd, shell=True, check=True)\n",
    "\n",
    "# Submit to Cromwell\n",
    "cromwell_url = \"http://localhost:8094/api/workflows/v1\"\n",
    "files = {\n",
    "    \"workflowSource\": wdl_path.open(\"rb\"),\n",
    "    \"workflowInputs\": json_path.open(\"rb\"),\n",
    "}\n",
    "print(\"Submitting to:\", cromwell_url)\n",
    "resp = requests.post(cromwell_url, files=files, headers={\"accept\": \"application/json\"})\n",
    "resp.raise_for_status()\n",
    "wf = resp.json()\n",
    "print(\"Response:\", wf)\n",
    "print(\"WF_ID:\", wf.get(\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor + logs\n",
    "wf_id = wf.get(\"id\")\n",
    "print(wf_id)\n",
    "\n",
    "print(wf_id)\n",
    "status = wait_for_wf(wf_id, poll_s=30, timeout_s=7200)\n",
    "print(\"Final status:\", status)\n",
    "\n",
    "pretty(get_wf_metadata(wf_id, include_keys=[\"failures\", \"callRoot\"]))\n",
    "fetch_task_logs_from_gcs(wf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
