{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mtDNA notebook\n",
    "### mitoClock "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET GLOBAL VARIABLES\n",
    "# NOTE: \n",
    "# The Cloud Life Sciences (GLS) API is expired !\n",
    "# Batch (GCB) migration in the All of Us (AOU) Workbench occurred in July 8, 2025\n",
    "# For migration details, \n",
    "# Refer to: https://cloud.google.com/batch/docs/migrate-to-batch-from-cloud-life-sciences. and here\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE_BUCKET = os.getenv(\"WORKSPACE_BUCKET\", \"\").rstrip(\"/\")\n",
    "GOOGLE_PROJECT = os.getenv(\"GOOGLE_PROJECT\", \"\")\n",
    "PET_SA_EMAIL = os.getenv(\"PET_SA_EMAIL\", \"\")\n",
    "\n",
    "outputFold = os.getenv(\"outputFold\", \"mtDNA_v25_pilot_5\")\n",
    "PORTID = int(os.getenv(\"PORTID\", \"8094\"))\n",
    "USE_MEM = int(os.getenv(\"USE_MEM\", \"32\"))\n",
    "SQL_DB_NAME = os.getenv(\"SQL_DB_NAME\", \"local_cromwell_run.db\")\n",
    "\n",
    "PROJECT_ROOT = Path(os.getenv(\"PROJECT_ROOT\", \"/mnt/f/research_drive/mtdna/leelab/mtDNA-analysis\")).resolve()\n",
    "\n",
    "print(\"WORKSPACE_BUCKET:\", WORKSPACE_BUCKET)\n",
    "print(\"GOOGLE_PROJECT:\", GOOGLE_PROJECT)\n",
    "print(\"PET_SA_EMAIL:\", PET_SA_EMAIL)\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"PORTID:\", PORTID, \"USE_MEM:\", USE_MEM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     15,
     25,
     28,
     32,
     37,
     40,
     49,
     57,
     60,
     64,
     67,
     70,
     73
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def run(cmd, check=True):\n",
    "    print(f\"$ {cmd}\")\n",
    "    return subprocess.run(\n",
    "        cmd, shell=True, check=check,\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "        text=True\n",
    "    ).stdout.strip()\n",
    "\n",
    "# --- Dependency checks ---\n",
    "checks = {\n",
    "    \"python\": sys.executable,\n",
    "    \"pip\": shutil.which(\"pip\") or shutil.which(\"pip3\") or \"\",\n",
    "    \"gsutil\": shutil.which(\"gsutil\") or \"\",\n",
    "    \"gcloud\": shutil.which(\"gcloud\") or \"\",\n",
    "    \"java\": shutil.which(\"java\") or \"\",\n",
    "    \"sdkman\": shutil.which(\"sdk\") or \"\",\n",
    "}\n",
    "\n",
    "print(\"Dependency check:\")\n",
    "for k, v in checks.items():\n",
    "    print(f\"  {k:8s} -> {v if v else 'MISSING'}\")\n",
    "\n",
    "if checks[\"java\"]:\n",
    "    print(\"\\nJava version:\")\n",
    "    print(run(\"java -version\", check=False))\n",
    "\n",
    "if checks[\"gcloud\"]:\n",
    "    print(\"\\nGcloud auth:\")\n",
    "    print(run(\"gcloud auth list --format='value(account)'\", check=False))\n",
    "\n",
    "# Optional: install pyhocon if missing\n",
    "try:\n",
    "    import pyhocon  # noqa: F401\n",
    "    print(\"\\npyhocon: OK\")\n",
    "except ImportError:\n",
    "    print(\"\\npyhocon: missing\")\n",
    "    # Uncomment to install\n",
    "    # print(run(f\"{sys.executable} -m pip install pyhocon\", check=True))\n",
    "\n",
    "# --- Optional bootstrap tools (comment out if not needed) ---\n",
    "home = Path.home()\n",
    "sdkman_dir = home / \".sdkman\"\n",
    "\n",
    "if not sdkman_dir.exists():\n",
    "    print(\"\\nInstalling SDKMAN...\")\n",
    "    run(\"curl -s https://get.sdkman.io -o install_sdkman.sh\", check=True)\n",
    "    run(\"bash install_sdkman.sh\", check=True)\n",
    "else:\n",
    "    print(\"\\nSDKMAN already installed.\")\n",
    "\n",
    "sdkman_init = sdkman_dir / \"bin\" / \"sdkman-init.sh\"\n",
    "if sdkman_init.exists():\n",
    "    run(f\"bash -lc 'source {sdkman_init} && sdk install java 17.0.8-tem || true'\", check=True)\n",
    "    run(f\"bash -lc 'source {sdkman_init} && sdk use java 17.0.8-tem'\", check=True)\n",
    "else:\n",
    "    print(\"SDKMAN init script not found; skipping Java install.\")\n",
    "\n",
    "# Cromwell/WOMtool 91\n",
    "if not Path(\"cromwell-91.jar\").exists():\n",
    "    print(\"Downloading cromwell-91.jar\")\n",
    "    run(\"curl -L https://github.com/broadinstitute/cromwell/releases/download/91/cromwell-91.jar -o cromwell-91.jar\", check=True)\n",
    "else:\n",
    "    print(\"cromwell-91.jar already present.\")\n",
    "\n",
    "if not Path(\"womtool-91.jar\").exists():\n",
    "    print(\"Downloading womtool-91.jar\")\n",
    "    run(\"curl -L https://github.com/broadinstitute/cromwell/releases/download/91/womtool-91.jar -o womtool-91.jar\", check=True)\n",
    "else:\n",
    "    print(\"womtool-91.jar already present.\")\n",
    "\n",
    "# Heap size to use in start_cromwell()\n",
    "CROMWELL_HEAP_GB = 32\n",
    "print(\"CROMWELL_HEAP_GB set to\", CROMWELL_HEAP_GB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROMWELL SERVER FUNCTIONS \n",
    "- START UP\n",
    "- CHECK STATUS\n",
    "- GET LOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     28,
     35,
     45,
     48,
     51,
     57,
     88,
     100,
     103,
     119,
     128,
     146,
     169,
     179,
     196
    ]
   },
   "outputs": [],
   "source": [
    "# some very helpful wrappers \n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# ---- Config ----\n",
    "CROMWELL_PORT = 8094\n",
    "CROMWELL_STATUS_URL = f\"http://localhost:{CROMWELL_PORT}/engine/v1/status\"\n",
    "CROMWELL_API = f\"http://localhost:{CROMWELL_PORT}/api/workflows/v1\"\n",
    "CROMWELL_CONF = Path(\"/home/jupyter/cromwell.conf\")\n",
    "CROMWELL_JAR = Path(\"cromwell-91.jar\")  # upgraded (81 was buggy and slow)\n",
    "SDKMAN_INIT = \"/home/jupyter/.sdkman/bin/sdkman-init.sh\"\n",
    "JAVA_VER = \"17.0.8-tem\"\n",
    "CROMWELL_HEAP_GB = 32  # new\n",
    "\n",
    "STDOUT_LOG = Path(\"cromwell_server_stdout.log\")\n",
    "STDERR_LOG = Path(\"cromwell_server_stderr.log\")\n",
    "PID_FILE = Path(\"cromwell_server.pid\")\n",
    "\n",
    "DB_BASE = Path(\"/home/jupyter/cromwell_db/local_cromwell_run.db\")\n",
    "DB_DATA = Path(str(DB_BASE) + \".data\")\n",
    "\n",
    "# ---- Variables ----\n",
    "_last_restart = 0\n",
    "\n",
    "# ---- Helpers ----\n",
    "def cromwell_up():\n",
    "    try:\n",
    "        r = requests.get(CROMWELL_STATUS_URL, timeout=2)\n",
    "        return r.ok\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def cromwell_pid_running():\n",
    "    if not PID_FILE.exists():\n",
    "        return False\n",
    "    try:\n",
    "        pid = int(PID_FILE.read_text().strip())\n",
    "        os.kill(pid, 0)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def cromwell_healthy():\n",
    "    return cromwell_pid_running() and cromwell_up()\n",
    "\n",
    "def cromwell_persistent_ok():\n",
    "    return DB_DATA.exists() and DB_DATA.stat().st_size > 0\n",
    "\n",
    "def cromwell_persistent_recent(max_age_s=300):\n",
    "    if not cromwell_persistent_ok():\n",
    "        return False\n",
    "    age = time.time() - DB_DATA.stat().st_mtime\n",
    "    return age <= max_age_s\n",
    "\n",
    "def start_cromwell():\n",
    "    if cromwell_healthy():\n",
    "        print(\"Cromwell already running and healthy.\")\n",
    "        if cromwell_persistent_ok():\n",
    "            print(\"Persistence check: DB data file OK.\")\n",
    "            if cromwell_persistent_recent():\n",
    "                print(\"DB was updated recently.\")\n",
    "        return\n",
    "\n",
    "    cmd = (\n",
    "        f\"bash -lc 'source {SDKMAN_INIT} && sdk use java {JAVA_VER} && \"\n",
    "        f\"nohup java -Xmx{CROMWELL_HEAP_GB}g \"\n",
    "        f\"-Dconfig.file={CROMWELL_CONF} -Dwebservice.port={CROMWELL_PORT} \"\n",
    "        f\"-jar {CROMWELL_JAR} server > {STDOUT_LOG} 2> {STDERR_LOG} & \"\n",
    "        f\"echo $! > {PID_FILE} && disown'\"\n",
    "    )\n",
    "    print(\"$\", cmd)\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "\n",
    "    for _ in range(30):\n",
    "        if cromwell_up():\n",
    "            print(\"Cromwell is up.\")\n",
    "            if cromwell_persistent_ok():\n",
    "                print(\"Persistence check: DB data file OK.\")\n",
    "                if cromwell_persistent_recent():\n",
    "                    print(\"DB was updated recently.\")\n",
    "            return\n",
    "        time.sleep(2)\n",
    "\n",
    "    raise RuntimeError(\"Cromwell did not start. Check stderr/stdout logs.\")\n",
    "\n",
    "def tail_logs(n=50):\n",
    "    if STDOUT_LOG.exists():\n",
    "        print(f\"--- {STDOUT_LOG} (last {n}) ---\")\n",
    "        print(\"\\n\".join(STDOUT_LOG.read_text().splitlines()[-n:]))\n",
    "    else:\n",
    "        print(f\"{STDOUT_LOG} not found.\")\n",
    "    if STDERR_LOG.exists():\n",
    "        print(f\"--- {STDERR_LOG} (last {n}) ---\")\n",
    "        print(\"\\n\".join(STDERR_LOG.read_text().splitlines()[-n:]))\n",
    "    else:\n",
    "        print(f\"{STDERR_LOG} not found.\")\n",
    "\n",
    "def pretty(obj):\n",
    "    print(json.dumps(obj, indent=2))\n",
    "\n",
    "def get_wf_status(wf_id, retries=10, sleep_s=2):\n",
    "    url = f\"{CROMWELL_API}/{wf_id}/status\"\n",
    "    last_err = None\n",
    "    for _ in range(retries):\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code == 404:\n",
    "            time.sleep(sleep_s)\n",
    "            continue\n",
    "        last_err = r\n",
    "        break\n",
    "    if last_err is not None:\n",
    "        last_err.raise_for_status()\n",
    "    raise RuntimeError(f\"Workflow {wf_id} not found after {retries} retries.\")\n",
    "\n",
    "def get_wf_metadata(wf_id, include_keys=None):\n",
    "    url = f\"{CROMWELL_API}/{wf_id}/metadata\"\n",
    "    if include_keys:\n",
    "        for k in include_keys:\n",
    "            url += f\"&includeKey={k}\" if \"?\" in url else f\"?includeKey={k}\"\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def wait_for_wf(wf_id, poll_s=5, timeout_s=600):\n",
    "    global _last_restart\n",
    "    deadline = time.time() + timeout_s\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            status = get_wf_status(wf_id).get(\"status\")\n",
    "            print(\"Status:\", status)\n",
    "            if status in (\"Succeeded\", \"Failed\", \"Aborted\"):\n",
    "                return status\n",
    "        except Exception:\n",
    "            now = time.time()\n",
    "            if now - _last_restart > 30:\n",
    "                print(\"Cromwell not reachable; restarting...\")\n",
    "                start_cromwell()\n",
    "                _last_restart = now\n",
    "        time.sleep(poll_s)\n",
    "    raise TimeoutError(f\"Workflow {wf_id} did not finish within {timeout_s}s\")\n",
    "\n",
    "def latest_workflow_id(wdl_name=None, status=None):\n",
    "    params = {\"page\": 1, \"pagesize\": 20}\n",
    "    if wdl_name:\n",
    "        params[\"name\"] = wdl_name\n",
    "    if status:\n",
    "        params[\"status\"] = status\n",
    "\n",
    "    r = requests.get(f\"{CROMWELL_API}/query\", params=params)\n",
    "    if r.status_code != 200:\n",
    "        payload = {\"page\": 1, \"pagesize\": 20}\n",
    "        if wdl_name:\n",
    "            payload[\"name\"] = wdl_name\n",
    "        if status:\n",
    "            payload[\"status\"] = status\n",
    "        r = requests.post(f\"{CROMWELL_API}/query\", json=payload)\n",
    "\n",
    "    r.raise_for_status()\n",
    "    results = r.json().get(\"results\", [])\n",
    "    if not results:\n",
    "        return None\n",
    "    results.sort(key=lambda x: x.get(\"submission\", \"\"), reverse=True)\n",
    "    return results[0].get(\"id\")\n",
    "\n",
    "def get_callroots(wf_id):\n",
    "    meta = get_wf_metadata(wf_id, include_keys=[\"callRoot\", \"calls\"])\n",
    "    callroots = []\n",
    "    calls = meta.get(\"calls\", {})\n",
    "    for call_name, entries in calls.items():\n",
    "        for e in entries:\n",
    "            if \"callRoot\" in e:\n",
    "                callroots.append((call_name, e[\"callRoot\"]))\n",
    "    return callroots\n",
    "\n",
    "def fetch_task_logs_from_gcs(wf_id, call_name=None):\n",
    "    callroots = get_callroots(wf_id)\n",
    "    if not callroots:\n",
    "        print(\"No callRoot entries found.\")\n",
    "        return\n",
    "\n",
    "    for name, root in callroots:\n",
    "        if call_name and call_name != name:\n",
    "            continue\n",
    "        stdout = f\"{root}/stdout\"\n",
    "        stderr = f\"{root}/stderr\"\n",
    "        print(f\"\\nCall: {name}\")\n",
    "        print(\"stdout:\", stdout)\n",
    "        print(\"stderr:\", stderr)\n",
    "        subprocess.run(f\"gsutil cat {stdout} | tail -n 50\", shell=True, check=False)\n",
    "        subprocess.run(f\"gsutil cat {stderr} | tail -n 50\", shell=True, check=False)\n",
    "\n",
    "def latest_workflow_id_gcs(workspace_bucket, workflow_name):\n",
    "    cmd = f\"gsutil ls -l {workspace_bucket}/workflows/cromwell-executions/{workflow_name}/\"\n",
    "    out = subprocess.check_output(cmd, shell=True, text=True)\n",
    "    lines = [l for l in out.splitlines() if l.strip().startswith(\"gs://\")]\n",
    "    if not lines:\n",
    "        return None\n",
    "    lines.sort()\n",
    "    latest = lines[-1].split()[-1].rstrip(\"/\")\n",
    "    return latest.split(\"/\")[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_cromwell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cromwell_up():\", cromwell_up())\n",
    "print(\"cromwell_pid_running():\", cromwell_pid_running())\n",
    "print(\"cromwell_healthy():\", cromwell_healthy())\n",
    "\n",
    "start_cromwell()\n",
    "\n",
    "print(\"cromwell_up():\", cromwell_up())\n",
    "print(\"cromwell_pid_running():\", cromwell_pid_running())\n",
    "print(\"cromwell_healthy():\", cromwell_healthy())\n",
    "\n",
    "print(\"cromwell_persistent_ok():\", cromwell_persistent_ok())\n",
    "print(\"cromwell_persistent_recent():\", cromwell_persistent_recent())\n",
    "\n",
    "tail_logs(20)\n",
    "\n",
    "# Optional: latest workflow ID (if any exist)\n",
    "print(\"latest_workflow_id():\", latest_workflow_id())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"latest_workflow_id():\", latest_workflow_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the bucket for latest ID by name \n",
    "latest_workflow_id_gcs(WORKSPACE_BUCKET, \"stage01_SubsetCramChrM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logs from tail \n",
    "N=50\n",
    "tail_logs(n=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  Run mode using Google Batch   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [IMPORTANT] Cromwell configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrades: add system tuning for throughput\n",
    "from pathlib import Path\n",
    "\n",
    "CROMWELL_DB = \"/home/jupyter/cromwell_db/local_cromwell_run.db\"\n",
    "Path(\"/home/jupyter/cromwell_db\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cromwell_conf = f\"\"\"include required(classpath(\"application\"))\n",
    "\n",
    "google {{\n",
    "  application-name = \"cromwell\"\n",
    "  auths = [{{\n",
    "    name = \"application_default\"\n",
    "    scheme = \"application_default\"\n",
    "  }}]\n",
    "}}\n",
    "\n",
    "system {{\n",
    "  new-workflow-poll-rate = 1\n",
    "  max-concurrent-workflows = 50\n",
    "  max-workflow-launch-count = 400\n",
    "  job-rate-control {{\n",
    "    jobs = 100\n",
    "    per = \"3 seconds\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "backend {{\n",
    "  default = \"GCPBATCH\"\n",
    "  providers {{\n",
    "    Local.config.root = \"/dev/null\"\n",
    "\n",
    "    GCPBATCH {{\n",
    "      actor-factory = \"cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory\"\n",
    "      config {{\n",
    "        project = \"{GOOGLE_PROJECT}\"\n",
    "        concurrent-job-limit = 20\n",
    "        root = \"{WORKSPACE_BUCKET}/workflows/cromwell-executions\"\n",
    "\n",
    "        virtual-private-cloud {{\n",
    "          network-name = \"projects/{GOOGLE_PROJECT}/global/networks/network\"\n",
    "          subnetwork-name = \"projects/{GOOGLE_PROJECT}/regions/us-central1/subnetworks/subnetwork\"\n",
    "        }}\n",
    "\n",
    "        batch {{\n",
    "          auth = \"application_default\"\n",
    "          compute-service-account = \"{PET_SA_EMAIL}\"\n",
    "          location = \"us-central1\"\n",
    "        }}\n",
    "\n",
    "        default-runtime-attributes {{\n",
    "          noAddress: true\n",
    "        }}\n",
    "\n",
    "        filesystems {{\n",
    "          gcs {{\n",
    "            auth = \"application_default\"\n",
    "          }}\n",
    "        }}\n",
    "      }}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\n",
    "database {{\n",
    "  profile = \"slick.jdbc.HsqldbProfile$\"\n",
    "  insert-batch-size = 6000\n",
    "  db {{\n",
    "    driver = \"org.hsqldb.jdbcDriver\"\n",
    "    url = \"jdbc:hsqldb:file:{CROMWELL_DB};shutdown=false;hsqldb.default_table_type=cached;hsqldb.tx=mvcc;hsqldb.large_data=true;hsqldb.lob_compressed=true;hsqldb.script_format=3;hsqldb.result_max_memory_rows=20000\"\n",
    "    connectionTimeout = 300000\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "Path(\"/home/jupyter/cromwell.conf\").write_text(cromwell_conf)\n",
    "print(\"Wrote /home/jupyter/cromwell.conf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/cromwell.conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/cromwell.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WDL Testing\n",
    "### Hello World "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small tutorial on how to submit jobs for processing \n",
    "from pathlib import Path\n",
    "test_dir = Path(\"./WDL/test\")\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created: {test_dir.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "wdl_text = \"\"\"\\\n",
    "version 1.0\n",
    "\n",
    "workflow HelloWorld {\n",
    "  call HelloTask\n",
    "  output {\n",
    "    String msg = HelloTask.out\n",
    "  }\n",
    "}\n",
    "\n",
    "task HelloTask {\n",
    "  input {\n",
    "    String name\n",
    "  }\n",
    "  command <<<\n",
    "    echo \"Hello, ~{name}!\"\n",
    "  >>>\n",
    "  output {\n",
    "    String out = read_string(stdout())\n",
    "  }\n",
    "  runtime {\n",
    "    docker: \"ubuntu:22.04\"\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "wdl_path = Path(\"./WDL/test/hello.wdl\")\n",
    "wdl_path.write_text(wdl_text)\n",
    "print(f\"Wrote: {wdl_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "inputs = {\n",
    "  \"HelloWorld.HelloTask.name\": \"World\"\n",
    "}\n",
    "\n",
    "json_path = Path(\"./WDL/test/hello.inputs.json\")\n",
    "json_path.write_text(json.dumps(inputs, indent=2) + \"\\n\")\n",
    "print(f\"Wrote: {json_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# from pathlib import Path\n",
    "# import requests\n",
    "\n",
    "# wdl_path = Path(\"./WDL/test/hello.wdl\")\n",
    "# json_path = Path(\"./WDL/test/hello.inputs.json\")\n",
    "\n",
    "# # Validate WDL with womtool (Java 17)\n",
    "# validate_cmd = (\n",
    "#     \"bash -lc 'source /home/jupyter/.sdkman/bin/sdkman-init.sh \"\n",
    "#     \"&& sdk use java 17.0.8-tem \"\n",
    "#     \"&& java -jar womtool-91.jar validate \"\n",
    "#     f\"{wdl_path}'\"\n",
    "# )\n",
    "# print(\"$\", validate_cmd)\n",
    "# subprocess.run(validate_cmd, shell=True, check=True)\n",
    "\n",
    "# # Submit to Cromwell\n",
    "# cromwell_url = \"http://localhost:8094/api/workflows/v1\"\n",
    "# files = {\n",
    "#     \"workflowSource\": wdl_path.open(\"rb\"),\n",
    "#     \"workflowInputs\": json_path.open(\"rb\"),\n",
    "# }\n",
    "# print(\"Submitting to:\", cromwell_url)\n",
    "# resp = requests.post(cromwell_url, files=files, headers={\"accept\": \"application/json\"})\n",
    "# resp.raise_for_status()\n",
    "# wf = resp.json()\n",
    "# print(\"Response:\", wf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time \n",
    "# wf_id = wf.get(\"id\")\n",
    "\n",
    "# start_time = time.perf_counter()\n",
    "# status = wait_for_wf(wf_id)\n",
    "# print(\"Final status:\", status)\n",
    "# fetch_task_logs_from_gcs(wf_id)\n",
    "# end_time = time.perf_counter()\n",
    "\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download or Generate Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mtdna metadata \n",
    "# Contains age, gender, sex and .cram/ .crai paths in google workspace\n",
    "import gzip\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def ensure_mtdna_tsv(download=False, print_head=False):\n",
    "    out_dir = Path(\"data/metadata\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/Kaychewe/mtDNA-analysis/meta/mtdna_mitoclock_aou_dataset_36246309_person_age_gender_crams.tsv.gz\"\n",
    "    path = out_dir / \"mtdna_mitoclock_aou_dataset_36246309_person_age_gender_crams.tsv.gz\"\n",
    "\n",
    "    if download or not path.exists():\n",
    "        subprocess.run([\"curl\", \"-L\", url, \"-o\", str(path)], check=True)\n",
    "        print(\"Downloaded:\", path.resolve())\n",
    "    else:\n",
    "        print(\"Using existing:\", path.resolve())\n",
    "\n",
    "    # gzip check + header\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        header = f.readline().strip()\n",
    "        first = f.readline().strip() if print_head else None\n",
    "\n",
    "    print(\"Gzip OK.\")\n",
    "    print(\"Header:\", header)\n",
    "    if print_head:\n",
    "        print(\"First row:\", first)\n",
    "\n",
    "    return path, header\n",
    "\n",
    "# Example usage:\n",
    "#ensure_mtdna_tsv(download=False, print_head=True)\n",
    "\n",
    "def load_and_sort_by_person_id(path, limit=None):\n",
    "    with gzip.open(path, \"rt\") as f:\n",
    "        header = f.readline().strip().split(\"\\t\")\n",
    "        rows = [line.strip().split(\"\\t\") for line in f if line.strip()]\n",
    "\n",
    "    # find person_id column\n",
    "    pid_idx = header.index(\"person_id\")\n",
    "    rows.sort(key=lambda r: int(r[pid_idx]))\n",
    "\n",
    "    if limit:\n",
    "        rows = rows[:limit]\n",
    "\n",
    "    return header, rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expecting age, gender, sex, cram, cram_id \n",
    "path = ensure_mtdna_tsv(download=False, print_head=True)[0]\n",
    "header, rows = load_and_sort_by_person_id(path, limit=2)\n",
    "print(\"Header:\", header)\n",
    "print(\"First 10 rows:\")\n",
    "for r in rows:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = ensure_mtdna_tsv(download=False, print_head=True)[0]\n",
    "df = pd.read_csv(path, sep=\"\\t\", compression=\"gzip\")\n",
    "df = df.sort_values(\"person_id\").reset_index(drop=True)\n",
    "print(df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 01: CRAM -> BAM/SAM (chrM + NUMT)\n",
    "Goal:\n",
    "- Start from a WGS CRAM and subset to chrM + NUMT intervals.\n",
    "- Clean and standardize reads (remove broken mates, revert, mark duplicates, sort).\n",
    "- Produce final outputs for downstream analysis.\n",
    "\n",
    "Inputs:\n",
    "- CRAM/CRAI for one sample.\n",
    "- Reference FASTA + index + dict.\n",
    "- chrM interval list + NUMT interval list.\n",
    "- Optional metadata (age/sex) to label outputs.\n",
    "\n",
    "Outputs:\n",
    "- Final processed BAM + BAI\n",
    "- Final SAM (from processed BAM)\n",
    "- Unmapped BAM\n",
    "- Duplicate metrics + coverage stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Select sample ----\n",
    "SELECT_PERSON_ID = 1000004\n",
    "SELECT_ROW_INDEX = 0\n",
    "\n",
    "if SELECT_PERSON_ID is not None:\n",
    "    row = df.loc[df[\"person_id\"] == SELECT_PERSON_ID].iloc[0]\n",
    "else:\n",
    "    row = df.iloc[SELECT_ROW_INDEX]\n",
    "\n",
    "sample_id = str(row[\"person_id\"])\n",
    "age = str(row[\"age\"]) if \"age\" in row and not pd.isna(row[\"age\"]) else None\n",
    "sex = str(row[\"sex_at_birth\"]) if \"sex_at_birth\" in row and not pd.isna(row[\"sex_at_birth\"]) else None\n",
    "\n",
    "# ---- Required reference paths ----\n",
    "ref_fasta = \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\"\n",
    "\n",
    "# ---- Build inputs ----\n",
    "inputs = {\n",
    "    \"stage01_SubsetCramChrM.sample_id\": sample_id,\n",
    "    \"stage01_SubsetCramChrM.age\": age,\n",
    "    \"stage01_SubsetCramChrM.sex\": sex,\n",
    "    \"stage01_SubsetCramChrM.input_cram\": row[\"cram_uri\"],\n",
    "    \"stage01_SubsetCramChrM.input_crai\": row[\"cram_index_uri\"],\n",
    "    \"stage01_SubsetCramChrM.ref_fasta\": ref_fasta,\n",
    "    \"stage01_SubsetCramChrM.docker\": \"kchewe/mtdna-samtools:conda-latest\",\n",
    "}\n",
    "\n",
    "out_path = Path(\"./WDL/s001/stage01_SubsetCramChrM.inputs.json\")\n",
    "out_path.write_text(json.dumps(inputs, indent=2) + \"\\n\")\n",
    "print(\"Wrote:\", out_path.resolve())\n",
    "print(\"Selected sample:\", sample_id, \"age:\", age, \"sex:\", sex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect JSON \n",
    "!cat /home/jupyter/workspaces/mtdnaheteroplasmyandaginganalysis/WDL/s001/stage01_SubsetCramChrM.inputs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -u $GOOGLE_PROJECT ls -l gs://gcp-public-data--broad-references/hg38/v0/chrM/chrM.hg38.interval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "wdl_text = \"\"\"\\\n",
    "version 1.0\n",
    "\n",
    "workflow stage01_SubsetCramChrM {\n",
    "  meta {\n",
    "    description: \"Lightweight samtools chrM-only subset: emit BAM/BAI/SAM.\"\n",
    "  }\n",
    "\n",
    "  input {\n",
    "    String sample_id\n",
    "    String? age\n",
    "    String? sex\n",
    "    File input_cram\n",
    "    File input_crai\n",
    "    File ref_fasta\n",
    "    String docker\n",
    "  }\n",
    "\n",
    "  call SubsetChrM_Samtools {\n",
    "    input:\n",
    "      sample_id = sample_id,\n",
    "      age = age,\n",
    "      sex = sex,\n",
    "      input_cram = input_cram,\n",
    "      input_crai = input_crai,\n",
    "      ref_fasta = ref_fasta,\n",
    "      docker = docker\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File final_bam = SubsetChrM_Samtools.final_bam\n",
    "    File final_bai = SubsetChrM_Samtools.final_bai\n",
    "    File final_sam = SubsetChrM_Samtools.final_sam\n",
    "  }\n",
    "}\n",
    "\n",
    "task SubsetChrM_Samtools {\n",
    "  input {\n",
    "    String sample_id\n",
    "    String? age\n",
    "    String? sex\n",
    "    File input_cram\n",
    "    File input_crai\n",
    "    File ref_fasta\n",
    "    String docker\n",
    "  }\n",
    "\n",
    "  String age_label = select_first([age, \"NA\"])\n",
    "  String sex_label = select_first([sex, \"NA\"])\n",
    "  String prefix = sample_id + \"_\" + age_label + \"_\" + sex_label + \"_chrM\"\n",
    "\n",
    "  command <<<\n",
    "    set -e\n",
    "    mkdir -p out\n",
    "    \n",
    "    # simplify the header \n",
    "    # samtools reheader command \n",
    "\n",
    "    # Subset chrM by contig name\n",
    "    samtools view -T \"~{ref_fasta}\" -b \"~{input_cram}\" chrM -o \"out/~{prefix}.bam\"\n",
    "\n",
    "    # Index BAM\n",
    "    samtools index \"out/~{prefix}.bam\"\n",
    "\n",
    "    # Export SAM\n",
    "    samtools view -h \"out/~{prefix}.bam\" > \"out/~{prefix}.sam\"\n",
    "  >>>\n",
    "\n",
    "  runtime {\n",
    "    docker: \"~{docker}\"\n",
    "    memory: \"8 GB\"\n",
    "    disks: \"local-disk 200 HDD\"\n",
    "    bootDiskSizeGb: 50\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File final_bam = \"out/~{prefix}.bam\"\n",
    "    File final_bai = \"out/~{prefix}.bam.bai\"\n",
    "    File final_sam = \"out/~{prefix}.sam\"\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "out_dir = Path(\"./WDL/s001\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "wdl_path = out_dir / \"stage01_SubsetCramChrM.wdl\"\n",
    "wdl_path.write_text(wdl_text)\n",
    "\n",
    "print(f\"Wrote: {wdl_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/jupyter/workspaces/mtdnaheteroplasmyandaginganalysis/WDL/s001/stage01_SubsetCramChrM.wdl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Stage01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# Submit stage01\n",
    "# Extract chrM\n",
    "# Updates from meeting 2/13\n",
    "# Simplified filenaming conventions\n",
    "# <sample_id>_<age>_<sex>.<bam|sam>\n",
    "\n",
    "start_cromwell()\n",
    "\n",
    "wdl_path = Path(\"./WDL/s001/stage01_SubsetCramChrM.wdl\")\n",
    "json_path = Path(\"./WDL/s001/stage01_SubsetCramChrM.inputs.json\")\n",
    "\n",
    "# Validate WDL\n",
    "validate_cmd = (\n",
    "    \"bash -lc 'source /home/jupyter/.sdkman/bin/sdkman-init.sh \"\n",
    "    \"&& sdk use java 17.0.8-tem \"\n",
    "    f\"&& java -jar womtool-91.jar validate {wdl_path}'\"\n",
    ")\n",
    "print(\"$\", validate_cmd)\n",
    "subprocess.run(validate_cmd, shell=True, check=True)\n",
    "\n",
    "# Submit to Cromwell\n",
    "cromwell_url = \"http://localhost:8094/api/workflows/v1\"\n",
    "files = {\n",
    "    \"workflowSource\": wdl_path.open(\"rb\"),\n",
    "    \"workflowInputs\": json_path.open(\"rb\"),\n",
    "}\n",
    "print(\"Submitting to:\", cromwell_url)\n",
    "resp = requests.post(cromwell_url, files=files, headers={\"accept\": \"application/json\"})\n",
    "resp.raise_for_status()\n",
    "wf = resp.json()\n",
    "print(\"Response:\", wf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor + logs\n",
    "wf_id = wf.get(\"id\")\n",
    "print(wf_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor Stage 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TIME MUST BE RECORDED \n",
    "# WRITE WDL TO INSPECT THE OUTPUTS\n",
    "# OR GSUITL \n",
    "# OR PYTHON COMMNADS IN JUPYTER \n",
    "# Note dynamic cormwell PID resolver works correctly \n",
    "print(wf_id)\n",
    "status = wait_for_wf(wf_id, poll_s=30, timeout_s=7200)\n",
    "print(\"Final status:\", status)\n",
    "\n",
    "pretty(get_wf_metadata(wf_id, include_keys=[\"failures\", \"callRoot\"]))\n",
    "fetch_task_logs_from_gcs(wf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to inpsect outputs for stage01 \n",
    "# expected BAMs, SAMs\n",
    "# Note use prior wf_id \n",
    "# stdout and stderr maybe avaiable\n",
    "# ! gsutil ls -lh gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage01_SubsetCramChrM/f38d54dd-1c21-481f-88c8-2582feede570/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get logs from tail\n",
    "# \n",
    "N=50\n",
    "tail_logs(n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOGS \n",
    "## RESOLVED\n",
    "## Failed run due to params \n",
    "## WF_ID=af51294d-673e-4100-a3ff-742d992417f3\n",
    "! gsutil -u $GOOGLE_PROJECT cat gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage01_SubsetCramChrM/af51294d-673e-4100-a3ff-742d992417f3/call-SubsetAndProcessChrM/stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -u $GOOGLE_PROJECT cat gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage01_SubsetCramChrM/1c02f1eb-3144-4afb-941c-bb18b8f6de0b/call-SubsetAndProcessChrM/script\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -u $GOOGLE_PROJECT ls gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage01_SubsetCramChrM/cb919b1f-204e-43ec-9d58-e23ca3c42b92/call-SubsetAndProcessChrM/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -u $GOOGLE_PROJECT ls gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage01_SubsetCramChrM/56f3c33b-2fcd-49d7-98fe-3b9ce4256695/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review BAMS and SAMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 02 mtdna VCFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "sep = \"\\n # --------------------------------- \"\n",
    "\n",
    "# ---- choose stage01 workflow id (latest) ----\n",
    "WF_ID = latest_workflow_id(\"stage01_SubsetCramChrM\")\n",
    "print(\"Stage01 WF_ID:\", WF_ID)\n",
    "\n",
    "# ---- pull outputs from Cromwell metadata ----\n",
    "meta = get_wf_metadata(WF_ID, include_keys=[\"outputs\"])\n",
    "outputs = meta.get(\"outputs\", {})\n",
    "print(sep)\n",
    "print(outputs)\n",
    "print(sep)\n",
    "\n",
    "# These keys match the samtools-only stage01 WDL\n",
    "input_bam = outputs.get(\"stage01_SubsetCramChrM.final_bam\")\n",
    "input_bai = outputs.get(\"stage01_SubsetCramChrM.final_bai\")\n",
    "\n",
    "if not input_bam or not input_bai:\n",
    "    raise ValueError(\"Could not find stage01 outputs in metadata. Check output key names.\")\n",
    "\n",
    "# Extract sample_id, age, sex from filename: <sample>_<age>_<sex>_chrM.bam\n",
    "fname = input_bam.split(\"/\")[-1]                # 1000004_85_Male_chrM.bam\n",
    "parts = fname.split(\"_\")\n",
    "sample_id = parts[0] if len(parts) > 0 else \"\"\n",
    "age = parts[1] if len(parts) > 1 else None\n",
    "sex = parts[2] if len(parts) > 2 else None\n",
    "\n",
    "print(input_bam)\n",
    "print(input_bai)\n",
    "print(sample_id, age, sex)\n",
    "\n",
    "# ---- mt reference paths ----\n",
    "mt_fasta = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.fasta\"\n",
    "mt_fasta_index = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.fasta.fai\"\n",
    "mt_dict = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.dict\"\n",
    "mt_interval_list = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/chrM.hg38.interval_list\"\n",
    "\n",
    "# ---- build inputs ----\n",
    "inputs = {\n",
    "    \"stage02_MtOnly.input_bam\": input_bam,\n",
    "    \"stage02_MtOnly.input_bai\": input_bai,\n",
    "    \"stage02_MtOnly.sample_id\": sample_id,\n",
    "    \"stage02_MtOnly.age\": age,\n",
    "    \"stage02_MtOnly.sex\": sex,\n",
    "    \"stage02_MtOnly.mt_fasta\": mt_fasta,\n",
    "    \"stage02_MtOnly.mt_fasta_index\": mt_fasta_index,\n",
    "    \"stage02_MtOnly.mt_dict\": mt_dict,\n",
    "    \"stage02_MtOnly.mt_interval_list\": mt_interval_list,\n",
    "    \"stage02_MtOnly.gatk_docker\": \"kchewe/mtdna-stage04:0.1.3\",\n",
    "    \"stage02_MtOnly.mem_gb\": 8,\n",
    "    \"stage02_MtOnly.n_cpu\": 2,\n",
    "}\n",
    "\n",
    "out_path = Path(\"./WDL/s002/stage02_MtOnly.inputs.json\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_path.write_text(json.dumps(inputs, indent=2) + \"\\n\")\n",
    "print(\"Wrote:\", out_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /home/jupyter/workspaces/mtdnaheteroplasmyandaginganalysis/WDL/s002/stage02_MtOnly.inputs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "wdl_text = \"\"\"\\\n",
    "version 1.0\n",
    "\n",
    "workflow stage02_MtOnly {\n",
    "  meta {\n",
    "    description: \"Stage02 simplified: mtDNA-only variant calling (Mutect2 + FilterMutectCalls).\"\n",
    "  }\n",
    "\n",
    "  input {\n",
    "    File input_bam\n",
    "    File input_bai\n",
    "    String sample_id\n",
    "    String? age\n",
    "    String? sex\n",
    "\n",
    "    File mt_fasta\n",
    "    File mt_fasta_index\n",
    "    File mt_dict\n",
    "    File mt_interval_list\n",
    "\n",
    "    String gatk_docker\n",
    "    Int? mem_gb\n",
    "    Int? n_cpu\n",
    "  }\n",
    "\n",
    "  call CallMtVariants {\n",
    "    input:\n",
    "      input_bam = input_bam,\n",
    "      input_bai = input_bai,\n",
    "      sample_id = sample_id,\n",
    "      age = age,\n",
    "      sex = sex,\n",
    "      mt_fasta = mt_fasta,\n",
    "      mt_fasta_index = mt_fasta_index,\n",
    "      mt_dict = mt_dict,\n",
    "      mt_interval_list = mt_interval_list,\n",
    "      gatk_docker = gatk_docker,\n",
    "      mem_gb = mem_gb,\n",
    "      n_cpu = n_cpu\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File out_vcf = CallMtVariants.out_vcf\n",
    "    File out_vcf_index = CallMtVariants.out_vcf_index\n",
    "  }\n",
    "}\n",
    "\n",
    "task CallMtVariants {\n",
    "  input {\n",
    "    File input_bam\n",
    "    File input_bai\n",
    "    String sample_id\n",
    "    String? age\n",
    "    String? sex\n",
    "\n",
    "    File mt_fasta\n",
    "    File mt_fasta_index\n",
    "    File mt_dict\n",
    "    File mt_interval_list\n",
    "\n",
    "    String gatk_docker\n",
    "    Int? mem_gb\n",
    "    Int? n_cpu\n",
    "  }\n",
    "\n",
    "  String age_label = select_first([age, \"NA\"])\n",
    "  String sex_label = select_first([sex, \"NA\"])\n",
    "  String prefix = sample_id + \"_\" + age_label + \"_\" + sex_label + \"_mt\"\n",
    "\n",
    "  command <<<\n",
    "    set -e\n",
    "    mkdir -p out\n",
    "\n",
    "    gatk Mutect2 \\\n",
    "      -R \"~{mt_fasta}\" \\\n",
    "      -I \"~{input_bam}\" \\\n",
    "      -L \"~{mt_interval_list}\" \\\n",
    "      --mitochondria-mode \\\n",
    "      -O \"out/~{prefix}.raw.vcf\"\n",
    "\n",
    "    gatk FilterMutectCalls \\\n",
    "      -R \"~{mt_fasta}\" \\\n",
    "      -V \"out/~{prefix}.raw.vcf\" \\\n",
    "      -O \"out/~{prefix}.filtered.vcf\"\n",
    "  >>>\n",
    "\n",
    "  runtime {\n",
    "    docker: \"~{gatk_docker}\"\n",
    "    memory: select_first([mem_gb, 8]) + \" GB\"\n",
    "    cpu: select_first([n_cpu, 2])\n",
    "    disks: \"local-disk 200 HDD\"\n",
    "    bootDiskSizeGb: 50\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File out_vcf = \"out/~{prefix}.filtered.vcf\"\n",
    "    File out_vcf_index = \"out/~{prefix}.filtered.vcf.idx\"\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "out_dir = Path(\"./WDL/s002\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "wdl_path = out_dir / \"stage02_MtOnly.wdl\"\n",
    "wdl_path.write_text(wdl_text)\n",
    "\n",
    "print(f\"Wrote: {wdl_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /home/jupyter/workspaces/mtdnaheteroplasmyandaginganalysis/WDL/s002/stage02_MtOnly.wdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Stage02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# Submit stage02\n",
    "# Simplified filenaming conventions\n",
    "# <sample_id>_<age>_<sex>_mt.filtered.vcf\n",
    "\n",
    "start_cromwell()\n",
    "\n",
    "wdl_path = Path(\"./WDL/s002/stage02_MtOnly.wdl\")\n",
    "json_path = Path(\"./WDL/s002/stage02_MtOnly.inputs.json\")\n",
    "\n",
    "# Validate WDL\n",
    "validate_cmd = (\n",
    "    \"bash -lc 'source /home/jupyter/.sdkman/bin/sdkman-init.sh \"\n",
    "    \"&& sdk use java 17.0.8-tem \"\n",
    "    f\"&& java -jar womtool-91.jar validate {wdl_path}'\"\n",
    ")\n",
    "print(\"$\", validate_cmd)\n",
    "subprocess.run(validate_cmd, shell=True, check=True)\n",
    "\n",
    "# Submit to Cromwell\n",
    "cromwell_url = \"http://localhost:8094/api/workflows/v1\"\n",
    "files = {\n",
    "    \"workflowSource\": wdl_path.open(\"rb\"),\n",
    "    \"workflowInputs\": json_path.open(\"rb\"),\n",
    "}\n",
    "print(\"Submitting to:\", cromwell_url)\n",
    "resp = requests.post(cromwell_url, files=files, headers={\"accept\": \"application/json\"})\n",
    "resp.raise_for_status()\n",
    "wf = resp.json()\n",
    "print(\"Response:\", wf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor Stage02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor + logs\n",
    "wf_id = wf.get(\"id\")\n",
    "print(wf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wf_id)\n",
    "status = wait_for_wf(wf_id, poll_s=30, timeout_s=7200)\n",
    "print(\"Final status:\", status)\n",
    "\n",
    "pretty(get_wf_metadata(wf_id, include_keys=[\"failures\", \"callRoot\"]))\n",
    "fetch_task_logs_from_gcs(wf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/**/a5f85f2f-65c6-442f-b21c-72624f3c3762/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cat gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage02_MtOnly/a5f85f2f-65c6-442f-b21c-72624f3c3762/call-CallMtVariants/out/1000004_85_Male_mt.filtered.vcf | awk 'BEGIN{OFS=\"\\t\"} !/^#/ {print $1,$2,$4,$5,$6,$7,$8,$9,$10}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cat gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage02_MtOnly/a5f85f2f-65c6-442f-b21c-72624f3c3762/call-CallMtVariants/out/1000004_85_Male_mt.filtered.vcf  | grep -v '^#' | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 03 Normalize/annotate/filter mt variants, apply VAF thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Stage02 outputs ----\n",
    "WF_ID = latest_workflow_id(\"stage02_MtOnly\")\n",
    "meta = get_wf_metadata(WF_ID, include_keys=[\"outputs\"])\n",
    "outputs = meta.get(\"outputs\", {})\n",
    "\n",
    "input_vcf = outputs.get(\"stage02_MtOnly.out_vcf\")\n",
    "input_vcf_index = outputs.get(\"stage02_MtOnly.out_vcf_index\")\n",
    "\n",
    "if not input_vcf or not input_vcf_index:\n",
    "    raise ValueError(\"Could not find stage02 outputs in metadata. Check output key names.\")\n",
    "\n",
    "# Extract sample_id/age/sex from filename\n",
    "fname = input_vcf.split(\"/\")[-1]   # e.g. 1000004_85_Male_mt.filtered.vcf\n",
    "parts = fname.split(\"_\")\n",
    "sample_id = parts[0] if len(parts) > 0 else \"\"\n",
    "age = parts[1] if len(parts) > 1 else None\n",
    "sex = parts[2] if len(parts) > 2 else None\n",
    "\n",
    "inputs = {\n",
    "    \"stage03_MtFilter.input_vcf\": input_vcf,\n",
    "    \"stage03_MtFilter.input_vcf_index\": input_vcf_index,\n",
    "    \"stage03_MtFilter.sample_id\": sample_id,\n",
    "    \"stage03_MtFilter.age\": age,\n",
    "    \"stage03_MtFilter.sex\": sex,\n",
    "    \"stage03_MtFilter.vaf_min\": 0.01,\n",
    "    \"stage03_MtFilter.docker\": \"kchewe/mtdna-tools:0.1.0\",\n",
    "}\n",
    "\n",
    "out_path = Path(\"./WDL/s003/stage03_MtFilter.inputs.json\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "out_path.write_text(json.dumps(inputs, indent=2) + \"\\n\")\n",
    "print(\"Wrote:\", out_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /home/jupyter/workspaces/mtdnaheteroplasmyandaginganalysis/WDL/s003/stage03_MtFilter.inputs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/**/out/**.vcf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage01_SubsetCramChrM/3cb6f132-8c37-4b31-a460-289d537506ee/call-SubsetChrM_Samtools/out/1000406_25_Female_chrM.sam | grep -v \"@head -n 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: directly compute heteroplasmy \n",
    "# TODO: collect into one dir by type \n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "wdl_text = \"\"\"\\\n",
    "version 1.0\n",
    "\n",
    "workflow stage03_MtFilter {\n",
    "  meta {\n",
    "    description: \"Stage03 simplified: filter mtDNA variants by VAF and emit VCF + TSV.\"\n",
    "  }\n",
    "\n",
    "  input {\n",
    "    File input_vcf\n",
    "    File input_vcf_index\n",
    "    String sample_id\n",
    "    String? age\n",
    "    String? sex\n",
    "    Float vaf_min = 0.01\n",
    "    String docker\n",
    "  }\n",
    "\n",
    "  call FilterToTsv {\n",
    "    input:\n",
    "      input_vcf = input_vcf,\n",
    "      input_vcf_index = input_vcf_index,\n",
    "      sample_id = sample_id,\n",
    "      age = age,\n",
    "      sex = sex,\n",
    "      vaf_min = vaf_min,\n",
    "      docker = docker\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File out_vcf = FilterToTsv.out_vcf\n",
    "    File out_tsv = FilterToTsv.out_tsv\n",
    "  }\n",
    "}\n",
    "\n",
    "task FilterToTsv {\n",
    "  input {\n",
    "    File input_vcf\n",
    "    File input_vcf_index\n",
    "    String sample_id\n",
    "    String? age\n",
    "    String? sex\n",
    "    Float vaf_min\n",
    "    String docker\n",
    "  }\n",
    "\n",
    "  String age_label = select_first([age, \"NA\"])\n",
    "  String sex_label = select_first([sex, \"NA\"])\n",
    "  String prefix = sample_id + \"_\" + age_label + \"_\" + sex_label + \"_mt\"\n",
    "\n",
    "  command <<<\n",
    "    set -e\n",
    "    mkdir -p out\n",
    "\n",
    "    # Filter by VAF (AF in FORMAT)\n",
    "    bcftools view -i \"FORMAT/AF>=~{vaf_min}\" \"~{input_vcf}\" -Oz -o \"out/~{prefix}.vaf~{vaf_min}.vcf.gz\"\n",
    "    tabix -p vcf \"out/~{prefix}.vaf~{vaf_min}.vcf.gz\"\n",
    "\n",
    "    # TSV output\n",
    "    bcftools query -f '%CHROM\\\\t%POS\\\\t%REF\\\\t%ALT\\\\t%QUAL\\\\t%FILTER\\\\t%INFO/DP\\\\t%FORMAT/AF\\\\n' \\\n",
    "      \"out/~{prefix}.vaf~{vaf_min}.vcf.gz\" > \"out/~{prefix}.vaf~{vaf_min}.tsv\"\n",
    "  >>>\n",
    "\n",
    "  runtime {\n",
    "    docker: \"~{docker}\"\n",
    "    memory: \"8 GB\"\n",
    "    cpu: 2\n",
    "    disks: \"local-disk 200 HDD\"\n",
    "    bootDiskSizeGb: 50\n",
    "  }\n",
    "\n",
    "  output {\n",
    "    File out_vcf = \"out/~{prefix}.vaf~{vaf_min}.vcf.gz\"\n",
    "    File out_tsv = \"out/~{prefix}.vaf~{vaf_min}.tsv\"\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "out_dir = Path(\"./WDL/s003\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "wdl_path = out_dir / \"stage03_MtFilter.wdl\"\n",
    "wdl_path.write_text(wdl_text)\n",
    "\n",
    "print(f\"Wrote: {wdl_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /home/jupyter/workspaces/mtdnaheteroplasmyandaginganalysis/WDL/s003/stage03_MtFilter.wdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Stage03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# Submit stage03\n",
    "# Outputs:\n",
    "# <sample_id>_<age>_<sex>_mt.vaf0.01.vcf.gz\n",
    "# <sample_id>_<age>_<sex>_mt.vaf0.01.tsv\n",
    "\n",
    "start_cromwell()\n",
    "\n",
    "wdl_path = Path(\"./WDL/s003/stage03_MtFilter.wdl\")\n",
    "json_path = Path(\"./WDL/s003/stage03_MtFilter.inputs.json\")\n",
    "\n",
    "# Validate WDL\n",
    "validate_cmd = (\n",
    "    \"bash -lc 'source /home/jupyter/.sdkman/bin/sdkman-init.sh \"\n",
    "    \"&& sdk use java 17.0.8-tem \"\n",
    "    f\"&& java -jar womtool-91.jar validate {wdl_path}'\"\n",
    ")\n",
    "print(\"$\", validate_cmd)\n",
    "subprocess.run(validate_cmd, shell=True, check=True)\n",
    "\n",
    "# Submit to Cromwell\n",
    "cromwell_url = \"http://localhost:8094/api/workflows/v1\"\n",
    "files = {\n",
    "    \"workflowSource\": wdl_path.open(\"rb\"),\n",
    "    \"workflowInputs\": json_path.open(\"rb\"),\n",
    "}\n",
    "print(\"Submitting to:\", cromwell_url)\n",
    "resp = requests.post(cromwell_url, files=files, headers={\"accept\": \"application/json\"})\n",
    "resp.raise_for_status()\n",
    "wf = resp.json()\n",
    "print(\"Response:\", wf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor Stage03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor + logs\n",
    "wf_id = wf.get(\"id\")\n",
    "print(wf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wf_id)\n",
    "status = wait_for_wf(wf_id, poll_s=30, timeout_s=7200)\n",
    "print(\"Final status:\", status)\n",
    "\n",
    "pretty(get_wf_metadata(wf_id, include_keys=[\"failures\", \"callRoot\"]))\n",
    "fetch_task_logs_from_gcs(wf_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage03_MtFilter/dc972013-ceb9-45a3-b4f6-76ef369887a2/call-FilterToTsv/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cat gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage03_MtFilter/dc972013-ceb9-45a3-b4f6-76ef369887a2/call-FilterToTsv/out/1000004_85_Male_mt.vaf0.01.tsv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cat gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/stage03_MtFilter/dc972013-ceb9-45a3-b4f6-76ef369887a2/call-FilterToTsv/out/1000004_85_Male_mt.vaf0.01.tsv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BATCH RUN (10 samples per age group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_cromwell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cromwell_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Batch Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Todo -------\n",
    "# compute heter\n",
    "\n",
    "# ---- age bins ----\n",
    "# age_bins = [(18,39), (40,59), (60,79), (80,120)]\n",
    "age_bins = [(60,79), (80,120)]\n",
    "\n",
    "# ---- config ----\n",
    "N_PER_BIN = 100\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "DOCKER_SAMTOOLS = \"kchewe/mtdna-samtools:conda-latest\"\n",
    "DOCKER_GATK = \"kchewe/mtdna-stage04:0.1.3\"\n",
    "DOCKER_TOOLS = \"kchewe/mtdna-tools:0.1.0\"\n",
    "\n",
    "ref_fasta = \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\"\n",
    "mt_fasta = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.fasta\"\n",
    "mt_fasta_index = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.fasta.fai\"\n",
    "mt_dict = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.dict\"\n",
    "mt_interval_list = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/chrM.hg38.interval_list\"\n",
    "\n",
    "# ---- output log ----\n",
    "log_path = Path(\"./WDL/batch_run_log.tsv\")\n",
    "if not log_path.exists():\n",
    "    log_path.write_text(\n",
    "        \"bin\\tperson_id\\t\"\n",
    "        \"s1_wf\\ts1_status\\ts1_bam\\ts1_bai\\t\"\n",
    "        \"s2_wf\\ts2_status\\ts2_vcf\\ts2_vcf_idx\\t\"\n",
    "        \"s3_wf\\ts3_status\\ts3_vcf\\ts3_tsv\\n\"\n",
    "    )\n",
    "\n",
    "def read_processed_ids():\n",
    "    processed = set()\n",
    "    lines = log_path.read_text().strip().splitlines()\n",
    "    for line in lines[1:]:\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) > 1:\n",
    "            processed.add(parts[1])\n",
    "    return processed\n",
    "\n",
    "def submit_workflow(wdl_path, inputs_path):\n",
    "    cromwell_url = \"http://localhost:8094/api/workflows/v1\"\n",
    "    files = {\n",
    "        \"workflowSource\": open(wdl_path, \"rb\"),\n",
    "        \"workflowInputs\": open(inputs_path, \"rb\"),\n",
    "    }\n",
    "    r = requests.post(cromwell_url, files=files, headers={\"accept\": \"application/json\"})\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"id\"]\n",
    "\n",
    "def wait_many(wf_map, label):\n",
    "    \"\"\"wf_map: sample_id -> wf_id\"\"\"\n",
    "    pending = dict(wf_map)\n",
    "    statuses = {}\n",
    "    while pending:\n",
    "        for sid, wf_id in list(pending.items()):\n",
    "            status = get_wf_status(wf_id).get(\"status\")\n",
    "            print(f\"[{label}] {sid} status={status}\")\n",
    "            if status in (\"Succeeded\", \"Failed\", \"Aborted\"):\n",
    "                statuses[sid] = status\n",
    "                pending.pop(sid)\n",
    "        if pending:\n",
    "            time.sleep(30)\n",
    "    return statuses\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "processed_ids = read_processed_ids()\n",
    "print(\"Already processed:\", len(processed_ids))\n",
    "\n",
    "# ---- main loop ----\n",
    "for lo, hi in age_bins:\n",
    "    print(f\"\\n=== AGE BIN {lo}-{hi} ===\")\n",
    "\n",
    "    group = df[(df[\"age\"] >= lo) & (df[\"age\"] <= hi)]\n",
    "    group = group[~group[\"person_id\"].astype(str).isin(processed_ids)]\n",
    "    group = group.head(N_PER_BIN)\n",
    "    print(f\"Selected {len(group)} samples after skipping processed\")\n",
    "\n",
    "    rows = list(group.to_dict(orient=\"records\"))\n",
    "\n",
    "    for batch in chunks(rows, BATCH_SIZE):\n",
    "        print(f\"\\n--- Submitting batch of {len(batch)} samples ---\")\n",
    "\n",
    "        # ---------- Stage01 batch ----------\n",
    "        s1_wf_map = {}\n",
    "        for row in batch:\n",
    "            sample_id = str(row[\"person_id\"])\n",
    "            age = str(row[\"age\"])\n",
    "            sex = str(row[\"sex_at_birth\"]) if \"sex_at_birth\" in row and not pd.isna(row[\"sex_at_birth\"]) else None\n",
    "\n",
    "            s1_inputs = {\n",
    "                \"stage01_SubsetCramChrM.sample_id\": sample_id,\n",
    "                \"stage01_SubsetCramChrM.age\": age,\n",
    "                \"stage01_SubsetCramChrM.sex\": sex,\n",
    "                \"stage01_SubsetCramChrM.input_cram\": row[\"cram_uri\"],\n",
    "                \"stage01_SubsetCramChrM.input_crai\": row[\"cram_index_uri\"],\n",
    "                \"stage01_SubsetCramChrM.ref_fasta\": ref_fasta,\n",
    "                \"stage01_SubsetCramChrM.docker\": DOCKER_SAMTOOLS,\n",
    "            }\n",
    "            s1_json = Path(\"./WDL/s001/stage01_SubsetCramChrM.inputs.json\")\n",
    "            s1_json.write_text(json.dumps(s1_inputs, indent=2) + \"\\n\")\n",
    "            print(f\"[Stage01] submitting {sample_id}\")\n",
    "            s1_wf = submit_workflow(\"./WDL/s001/stage01_SubsetCramChrM.wdl\", s1_json)\n",
    "            s1_wf_map[sample_id] = s1_wf\n",
    "            print(f\"[Stage01] {sample_id} WF_ID={s1_wf}\")\n",
    "\n",
    "        s1_status = wait_many(s1_wf_map, \"Stage01\")\n",
    "\n",
    "        # ---------- Stage02 batch ----------\n",
    "        s2_wf_map = {}\n",
    "        s1_outputs = {}\n",
    "        for row in batch:\n",
    "            sample_id = str(row[\"person_id\"])\n",
    "            age = str(row[\"age\"])\n",
    "            sex = str(row[\"sex_at_birth\"]) if \"sex_at_birth\" in row and not pd.isna(row[\"sex_at_birth\"]) else None\n",
    "\n",
    "            if s1_status.get(sample_id) != \"Succeeded\":\n",
    "                print(f\"[Stage02] skip {sample_id} (Stage01 {s1_status.get(sample_id)})\")\n",
    "                continue\n",
    "\n",
    "            s1_out = get_wf_metadata(s1_wf_map[sample_id], include_keys=[\"outputs\"])[\"outputs\"]\n",
    "            bam = s1_out[\"stage01_SubsetCramChrM.final_bam\"]\n",
    "            bai = s1_out[\"stage01_SubsetCramChrM.final_bai\"]\n",
    "            s1_outputs[sample_id] = (bam, bai)\n",
    "\n",
    "            s2_inputs = {\n",
    "                \"stage02_MtOnly.input_bam\": bam,\n",
    "                \"stage02_MtOnly.input_bai\": bai,\n",
    "                \"stage02_MtOnly.sample_id\": sample_id,\n",
    "                \"stage02_MtOnly.age\": age,\n",
    "                \"stage02_MtOnly.sex\": sex,\n",
    "                \"stage02_MtOnly.mt_fasta\": mt_fasta,\n",
    "                \"stage02_MtOnly.mt_fasta_index\": mt_fasta_index,\n",
    "                \"stage02_MtOnly.mt_dict\": mt_dict,\n",
    "                \"stage02_MtOnly.mt_interval_list\": mt_interval_list,\n",
    "                \"stage02_MtOnly.gatk_docker\": DOCKER_GATK,\n",
    "                \"stage02_MtOnly.mem_gb\": 8,\n",
    "                \"stage02_MtOnly.n_cpu\": 2,\n",
    "            }\n",
    "            s2_json = Path(\"./WDL/s002/stage02_MtOnly.inputs.json\")\n",
    "            s2_json.write_text(json.dumps(s2_inputs, indent=2) + \"\\n\")\n",
    "            print(f\"[Stage02] submitting {sample_id}\")\n",
    "            s2_wf = submit_workflow(\"./WDL/s002/stage02_MtOnly.wdl\", s2_json)\n",
    "            s2_wf_map[sample_id] = s2_wf\n",
    "            print(f\"[Stage02] {sample_id} WF_ID={s2_wf}\")\n",
    "\n",
    "        s2_status = wait_many(s2_wf_map, \"Stage02\")\n",
    "\n",
    "        # ---------- Stage03 batch ----------\n",
    "        s3_wf_map = {}\n",
    "        s2_outputs = {}\n",
    "        for row in batch:\n",
    "            sample_id = str(row[\"person_id\"])\n",
    "            age = str(row[\"age\"])\n",
    "            sex = str(row[\"sex_at_birth\"]) if \"sex_at_birth\" in row and not pd.isna(row[\"sex_at_birth\"]) else None\n",
    "\n",
    "            if s2_status.get(sample_id) != \"Succeeded\":\n",
    "                print(f\"[Stage03] skip {sample_id} (Stage02 {s2_status.get(sample_id)})\")\n",
    "                continue\n",
    "\n",
    "            s2_out = get_wf_metadata(s2_wf_map[sample_id], include_keys=[\"outputs\"])[\"outputs\"]\n",
    "            vcf = s2_out[\"stage02_MtOnly.out_vcf\"]\n",
    "            vcf_idx = s2_out[\"stage02_MtOnly.out_vcf_index\"]\n",
    "            s2_outputs[sample_id] = (vcf, vcf_idx)\n",
    "\n",
    "            s3_inputs = {\n",
    "                \"stage03_MtFilter.input_vcf\": vcf,\n",
    "                \"stage03_MtFilter.input_vcf_index\": vcf_idx,\n",
    "                \"stage03_MtFilter.sample_id\": sample_id,\n",
    "                \"stage03_MtFilter.age\": age,\n",
    "                \"stage03_MtFilter.sex\": sex,\n",
    "                \"stage03_MtFilter.vaf_min\": 0.01,\n",
    "                \"stage03_MtFilter.docker\": DOCKER_TOOLS,\n",
    "            }\n",
    "            s3_json = Path(\"./WDL/s003/stage03_MtFilter.inputs.json\")\n",
    "            s3_json.write_text(json.dumps(s3_inputs, indent=2) + \"\\n\")\n",
    "            print(f\"[Stage03] submitting {sample_id}\")\n",
    "            s3_wf = submit_workflow(\"./WDL/s003/stage03_MtFilter.wdl\", s3_json)\n",
    "            s3_wf_map[sample_id] = s3_wf\n",
    "            print(f\"[Stage03] {sample_id} WF_ID={s3_wf}\")\n",
    "\n",
    "        s3_status = wait_many(s3_wf_map, \"Stage03\")\n",
    "\n",
    "        # ---------- Logging ----------\n",
    "        for row in batch:\n",
    "            sample_id = str(row[\"person_id\"])\n",
    "            s1_wf = s1_wf_map.get(sample_id, \"\")\n",
    "            s2_wf = s2_wf_map.get(sample_id, \"\")\n",
    "            s3_wf = s3_wf_map.get(sample_id, \"\")\n",
    "\n",
    "            s1_stat = s1_status.get(sample_id, \"\")\n",
    "            s2_stat = s2_status.get(sample_id, \"\")\n",
    "            s3_stat = s3_status.get(sample_id, \"\")\n",
    "\n",
    "            bam, bai = s1_outputs.get(sample_id, (\"\", \"\"))\n",
    "            vcf, vcf_idx = s2_outputs.get(sample_id, (\"\", \"\"))\n",
    "\n",
    "            out_vcf = \"\"\n",
    "            out_tsv = \"\"\n",
    "            if s3_stat == \"Succeeded\":\n",
    "                s3_out = get_wf_metadata(s3_wf_map[sample_id], include_keys=[\"outputs\"])[\"outputs\"]\n",
    "                out_vcf = s3_out.get(\"stage03_MtFilter.out_vcf\", \"\")\n",
    "                out_tsv = s3_out.get(\"stage03_MtFilter.out_tsv\", \"\")\n",
    "\n",
    "            line = f\"{lo}-{hi}\\t{sample_id}\\t{s1_wf}\\t{s1_stat}\\t{bam}\\t{bai}\\t{s2_wf}\\t{s2_stat}\\t{vcf}\\t{vcf_idx}\\t{s3_wf}\\t{s3_stat}\\t{out_vcf}\\t{out_tsv}\\n\"\n",
    "            log_path.write_text(log_path.read_text() + line)\n",
    "\n",
    "print(\"All bins done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Submission 10 samples per age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- age bins ----\n",
    "# decades: 18–19, 20s, 30s, ... 90s, 100–120\n",
    "age_bins = [(18,19)] + [(d, d+9) for d in range(20, 100, 10)] + [(100,120)]\n",
    "\n",
    "# ---- config ----\n",
    "N_PER_BIN = 100 # per sample group\n",
    "BATCH_SIZE = 20 # run in batches of 20\n",
    "POLL_S = 30\n",
    "HEARTBEAT_S = 300  # print a heartbeat every 5 min even if no status change\n",
    "\n",
    "DOCKER_SAMTOOLS = \"kchewe/mtdna-samtools:conda-latest\"\n",
    "DOCKER_GATK = \"kchewe/mtdna-stage04:0.1.3\"\n",
    "DOCKER_TOOLS = \"kchewe/mtdna-tools:0.1.0\"\n",
    "\n",
    "ref_fasta = \"gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\"\n",
    "mt_fasta = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.fasta\"\n",
    "mt_fasta_index = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.fasta.fai\"\n",
    "mt_dict = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/Homo_sapiens_assembly38.chrM.dict\"\n",
    "mt_interval_list = \"gs://gcp-public-data--broad-references/hg38/v0/chrM/chrM.hg38.interval_list\"\n",
    "\n",
    "# ---- output log ----\n",
    "log_path = Path(\"./WDL/batch_run_log.tsv\")\n",
    "if not log_path.exists():\n",
    "    log_path.write_text(\n",
    "        \"bin\\tperson_id\\t\"\n",
    "        \"s1_wf\\ts1_status\\ts1_bam\\ts1_bai\\t\"\n",
    "        \"s2_wf\\ts2_status\\ts2_vcf\\ts2_vcf_idx\\t\"\n",
    "        \"s3_wf\\ts3_status\\ts3_vcf\\ts3_tsv\\n\"\n",
    "    )\n",
    "\n",
    "def read_processed_ids():\n",
    "    processed = set()\n",
    "    lines = log_path.read_text().strip().splitlines()\n",
    "    for line in lines[1:]:\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) > 1:\n",
    "            processed.add(parts[1])\n",
    "    return processed\n",
    "\n",
    "def submit_workflow(wdl_path, inputs_path):\n",
    "    cromwell_url = \"http://localhost:8094/api/workflows/v1\"\n",
    "    files = {\n",
    "        \"workflowSource\": open(wdl_path, \"rb\"),\n",
    "        \"workflowInputs\": open(inputs_path, \"rb\"),\n",
    "    }\n",
    "    r = requests.post(cromwell_url, files=files, headers={\"accept\": \"application/json\"})\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"id\"]\n",
    "\n",
    "def wait_many(wf_map, label):\n",
    "    pending = dict(wf_map)\n",
    "    statuses = {}\n",
    "    last_seen = {sid: None for sid in pending}\n",
    "    last_hb = time.time()\n",
    "\n",
    "    while pending:\n",
    "        for sid, wf_id in list(pending.items()):\n",
    "            status = get_wf_status(wf_id).get(\"status\")\n",
    "            if status != last_seen[sid]:\n",
    "                print(f\"[{label}] {sid} status={status}\")\n",
    "                last_seen[sid] = status\n",
    "            if status in (\"Succeeded\", \"Failed\", \"Aborted\"):\n",
    "                statuses[sid] = status\n",
    "                pending.pop(sid)\n",
    "\n",
    "        now = time.time()\n",
    "        if pending and (now - last_hb) >= HEARTBEAT_S:\n",
    "            print(f\"[{label}] heartbeat: {len(pending)} still running\")\n",
    "            last_hb = now\n",
    "\n",
    "        if pending:\n",
    "            time.sleep(POLL_S)\n",
    "    return statuses\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "processed_ids = read_processed_ids()\n",
    "print(\"Already processed:\", len(processed_ids))\n",
    "\n",
    "# ---- main loop ----\n",
    "for lo, hi in age_bins:\n",
    "    print(f\"\\n=== AGE BIN {lo}-{hi} ===\")\n",
    "\n",
    "    group = df[(df[\"age\"] >= lo) & (df[\"age\"] <= hi)]\n",
    "    group = group[~group[\"person_id\"].astype(str).isin(processed_ids)]\n",
    "    group = group.head(N_PER_BIN)\n",
    "    print(f\"Selected {len(group)} samples after skipping processed\")\n",
    "\n",
    "    rows = list(group.to_dict(orient=\"records\"))\n",
    "\n",
    "    for batch in chunks(rows, BATCH_SIZE):\n",
    "        print(f\"\\n--- Submitting batch of {len(batch)} samples ---\")\n",
    "\n",
    "        # ---------- Stage01 batch ----------\n",
    "        s1_wf_map = {}\n",
    "        for row in batch:\n",
    "            sample_id = str(row[\"person_id\"])\n",
    "            age = str(row[\"age\"])\n",
    "            sex = str(row[\"sex_at_birth\"]) if \"sex_at_birth\" in row and not pd.isna(row[\"sex_at_birth\"]) else None\n",
    "\n",
    "            s1_inputs = {\n",
    "                \"stage01_SubsetCramChrM.sample_id\": sample_id,\n",
    "                \"stage01_SubsetCramChrM.age\": age,\n",
    "                \"stage01_SubsetCramChrM.sex\": sex,\n",
    "                \"stage01_SubsetCramChrM.input_cram\": row[\"cram_uri\"],\n",
    "                \"stage01_SubsetCramChrM.input_crai\": row[\"cram_index_uri\"],\n",
    "                \"stage01_SubsetCramChrM.ref_fasta\": ref_fasta,\n",
    "                \"stage01_SubsetCramChrM.docker\": DOCKER_SAMTOOLS,\n",
    "            }\n",
    "            s1_json = Path(\"./WDL/s001/stage01_SubsetCramChrM.inputs.json\")\n",
    "            s1_json.write_text(json.dumps(s1_inputs, indent=2) + \"\\n\")\n",
    "            print(f\"[Stage01] submitting {sample_id}\")\n",
    "            s1_wf = submit_workflow(\"./WDL/s001/stage01_SubsetCramChrM.wdl\", s1_json)\n",
    "            s1_wf_map[sample_id] = s1_wf\n",
    "            print(f\"[Stage01] {sample_id} WF_ID={s1_wf}\")\n",
    "\n",
    "        s1_status = wait_many(s1_wf_map, \"Stage01\")\n",
    "\n",
    "        # ---------- Stage02 batch ----------\n",
    "        s2_wf_map = {}\n",
    "        s1_outputs = {}\n",
    "        for row in batch:\n",
    "            sample_id = str(row[\"person_id\"])\n",
    "            age = str(row[\"age\"])\n",
    "            sex = str(row[\"sex_at_birth\"]) if \"sex_at_birth\" in row and not pd.isna(row[\"sex_at_birth\"]) else None\n",
    "\n",
    "            if s1_status.get(sample_id) != \"Succeeded\":\n",
    "                print(f\"[Stage02] skip {sample_id} (Stage01 {s1_status.get(sample_id)})\")\n",
    "                continue\n",
    "\n",
    "            s1_out = get_wf_metadata(s1_wf_map[sample_id], include_keys=[\"outputs\"])[\"outputs\"]\n",
    "            bam = s1_out[\"stage01_SubsetCramChrM.final_bam\"]\n",
    "            bai = s1_out[\"stage01_SubsetCramChrM.final_bai\"]\n",
    "            s1_outputs[sample_id] = (bam, bai)\n",
    "\n",
    "            s2_inputs = {\n",
    "                \"stage02_MtOnly.input_bam\": bam,\n",
    "                \"stage02_MtOnly.input_bai\": bai,\n",
    "                \"stage02_MtOnly.sample_id\": sample_id,\n",
    "                \"stage02_MtOnly.age\": age,\n",
    "                \"stage02_MtOnly.sex\": sex,\n",
    "                \"stage02_MtOnly.mt_fasta\": mt_fasta,\n",
    "                \"stage02_MtOnly.mt_fasta_index\": mt_fasta_index,\n",
    "                \"stage02_MtOnly.mt_dict\": mt_dict,\n",
    "                \"stage02_MtOnly.mt_interval_list\": mt_interval_list,\n",
    "                \"stage02_MtOnly.gatk_docker\": DOCKER_GATK,\n",
    "                \"stage02_MtOnly.mem_gb\": 8,\n",
    "                \"stage02_MtOnly.n_cpu\": 2,\n",
    "            }\n",
    "            s2_json = Path(\"./WDL/s002/stage02_MtOnly.inputs.json\")\n",
    "            s2_json.write_text(json.dumps(s2_inputs, indent=2) + \"\\n\")\n",
    "            print(f\"[Stage02] submitting {sample_id}\")\n",
    "            s2_wf = submit_workflow(\"./WDL/s002/stage02_MtOnly.wdl\", s2_json)\n",
    "            s2_wf_map[sample_id] = s2_wf\n",
    "            print(f\"[Stage02] {sample_id} WF_ID={s2_wf}\")\n",
    "\n",
    "        s2_status = wait_many(s2_wf_map, \"Stage02\")\n",
    "\n",
    "        # ---------- Stage03 batch ----------\n",
    "        s3_wf_map = {}\n",
    "        s2_outputs = {}\n",
    "        for row in batch:\n",
    "            sample_id = str(row[\"person_id\"])\n",
    "            age = str(row[\"age\"])\n",
    "            sex = str(row[\"sex_at_birth\"]) if \"sex_at_birth\" in row and not pd.isna(row[\"sex_at_birth\"]) else None\n",
    "\n",
    "            if s2_status.get(sample_id) != \"Succeeded\":\n",
    "                print(f\"[Stage03] skip {sample_id} (Stage02 {s2_status.get(sample_id)})\")\n",
    "                continue\n",
    "\n",
    "            s2_out = get_wf_metadata(s2_wf_map[sample_id], include_keys=[\"outputs\"])[\"outputs\"]\n",
    "            vcf = s2_out[\"stage02_MtOnly.out_vcf\"]\n",
    "            vcf_idx = s2_out[\"stage02_MtOnly.out_vcf_index\"]\n",
    "            s2_outputs[sample_id] = (vcf, vcf_idx)\n",
    "\n",
    "            s3_inputs = {\n",
    "                \"stage03_MtFilter.input_vcf\": vcf,\n",
    "                \"stage03_MtFilter.input_vcf_index\": vcf_idx,\n",
    "                \"stage03_MtFilter.sample_id\": sample_id,\n",
    "                \"stage03_MtFilter.age\": age,\n",
    "                \"stage03_MtFilter.sex\": sex,\n",
    "                \"stage03_MtFilter.vaf_min\": 0.01,\n",
    "                \"stage03_MtFilter.docker\": DOCKER_TOOLS,\n",
    "            }\n",
    "            s3_json = Path(\"./WDL/s003/stage03_MtFilter.inputs.json\")\n",
    "            s3_json.write_text(json.dumps(s3_inputs, indent=2) + \"\\n\")\n",
    "            print(f\"[Stage03] submitting {sample_id}\")\n",
    "            s3_wf = submit_workflow(\"./WDL/s003/stage03_MtFilter.wdl\", s3_json)\n",
    "            s3_wf_map[sample_id] = s3_wf\n",
    "            print(f\"[Stage03] {sample_id} WF_ID={s3_wf}\")\n",
    "\n",
    "        s3_status = wait_many(s3_wf_map, \"Stage03\")\n",
    "\n",
    "        # ---------- Logging ----------\n",
    "        for row in batch:\n",
    "            sample_id = str(row[\"person_id\"])\n",
    "            s1_wf = s1_wf_map.get(sample_id, \"\")\n",
    "            s2_wf = s2_wf_map.get(sample_id, \"\")\n",
    "            s3_wf = s3_wf_map.get(sample_id, \"\")\n",
    "\n",
    "            s1_stat = s1_status.get(sample_id, \"\")\n",
    "            s2_stat = s2_status.get(sample_id, \"\")\n",
    "            s3_stat = s3_status.get(sample_id, \"\")\n",
    "\n",
    "            bam, bai = s1_outputs.get(sample_id, (\"\", \"\"))\n",
    "            vcf, vcf_idx = s2_outputs.get(sample_id, (\"\", \"\"))\n",
    "\n",
    "            out_vcf = \"\"\n",
    "            out_tsv = \"\"\n",
    "            if s3_stat == \"Succeeded\":\n",
    "                s3_out = get_wf_metadata(s3_wf_map[sample_id], include_keys=[\"outputs\"])[\"outputs\"]\n",
    "                out_vcf = s3_out.get(\"stage03_MtFilter.out_vcf\", \"\")\n",
    "                out_tsv = s3_out.get(\"stage03_MtFilter.out_tsv\", \"\")\n",
    "\n",
    "            line = f\"{lo}-{hi}\\t{sample_id}\\t{s1_wf}\\t{s1_stat}\\t{bam}\\t{bai}\\t{s2_wf}\\t{s2_stat}\\t{vcf}\\t{vcf_idx}\\t{s3_wf}\\t{s3_stat}\\t{out_vcf}\\t{out_tsv}\\n\"\n",
    "            log_path.write_text(log_path.read_text() + line)\n",
    "\n",
    "print(\"All bins done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls  gs://fc-secure-76d68a64-00aa-40a7-b2c5-ca956db2719b/workflows/cromwell-executions/**/6b17c985-d6c2-46b0-9d00-ece5058d5d3a/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cromwell_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mtDNA Variant EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tsv \n",
    "# parse age, sex, gender \n",
    "# parse vcfs \n",
    "# parse tsv \n",
    "mtDNA_data_path = \"./WDL/batch_run_log.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
